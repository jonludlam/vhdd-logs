[20131218T17:44:20.741Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|0||watchdog] (Re)starting vhdd...
[20131218T17:44:20.742Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|0||watchdog] Child vhdd is: 26323
[20131218T17:44:20.742Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|0||http] Establishing inet domain server
[20131218T17:44:20.744Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|0||attachments] Ignoring ENOENT while reading attachments file
[20131218T17:44:20.745Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|9 inet-rpc||http] Request { frame = false; method = POST; uri = /internal; query = [  ]; content_length = [ 56 ]; transfer encoding = ; version = 1.0; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = ; user_agent = xen-api-libs/1.0 }
[20131218T17:44:20.745Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|9 inet-rpc||vhdd] Internal handler
[20131218T17:44:20.745Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|9 inet-rpc||vhdd] Call={"method": "Debug.get_pid", "params": [null], "id": 928}
[20131218T17:44:20.745Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|9 inet-rpc||vhdd] Response: {"result": 26323, "error": null, "id": 0}
[20131218T17:44:20.746Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|12 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 204 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:20.746Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|12 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:20.747Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|12 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>SR.list</methodName><params><param><value><struct><member><name>dbg</name><value>wait_for_start</value></member></struct></value></param></params></methodCall>
[20131218T17:44:20.747Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|12 inet-rpc|9dde2475-af6a-4b66-a005-9f1586907814|vhdd] Response: (omitted)
[20131218T17:44:20.748Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|13 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 574 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:20.748Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|13 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:20.748Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|13 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>SR.create</methodName><params><param><value><struct><member><name>dbg</name><value>create_and_attach</value></member><member><name>sr</name><value>1</value></member><member><name>device_config</name><value><struct><member><name>SRmaster</name><value>true</value></member><member><name>device</name><value>/dev/dummy</value></member><member><name>reservation_mode</name><value>thin</value></member></struct></value></member><member><name>physical_size</name><value>0</value></member></struct></value></param></params></methodCall>
[20131218T17:44:20.749Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|13 inet-rpc|e3493549-3aff-4a18-a712-a24cafb16de2|mlvm] write_label_and_pv_header:
PV header:
pvh_id: XHZVvY-fGo4-fBdW-BD3c-xAOh-F9o3-0j5s1z
pvh_device_size: 1099511627776
pvh_areas1: {offset=10551296,size=0}
pvh_areas2: {offset=4096,size=10547200}

[20131218T17:44:20.786Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|13 inet-rpc|e3493549-3aff-4a18-a712-a24cafb16de2|mlvm] Writing MDA header
[20131218T17:44:20.839Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|13 inet-rpc|e3493549-3aff-4a18-a712-a24cafb16de2|mlvm] Writing: checksum: 0
magic:  LVM2 x[5A%r0N*>
version: 1
start: 4096
size: 10485760
raw_locns:[{offset:512,size:0,checksum:0,filler:0}]

[20131218T17:44:20.843Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|13 inet-rpc|e3493549-3aff-4a18-a712-a24cafb16de2|mlvm] PVs created
[20131218T17:44:20.843Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|13 inet-rpc|e3493549-3aff-4a18-a712-a24cafb16de2|mlvm] write_label_and_pv_header:
PV header:
pvh_id: XHZVvY-fGo4-fBdW-BD3c-xAOh-F9o3-0j5s1z
pvh_device_size: 1099511627776
pvh_areas1: {offset=10551296,size=0}
pvh_areas2: {offset=4096,size=10547200}

[20131218T17:44:20.850Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|13 inet-rpc|e3493549-3aff-4a18-a712-a24cafb16de2|mlvm] Writing MDA header
[20131218T17:44:20.850Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|13 inet-rpc|e3493549-3aff-4a18-a712-a24cafb16de2|mlvm] Writing: checksum: 0
magic:  LVM2 x[5A%r0N*>
version: 1
start: 4096
size: 10485760
raw_locns:[{offset:512,size:511,checksum:1723583840,filler:0}]

[20131218T17:44:20.851Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|13 inet-rpc|e3493549-3aff-4a18-a712-a24cafb16de2|mlvm] VG created
[20131218T17:44:20.851Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|13 inet-rpc|e3493549-3aff-4a18-a712-a24cafb16de2|vhdd] Response: (omitted)
[20131218T17:44:20.856Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 515 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:20.856Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:20.856Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>SR.attach</methodName><params><param><value><struct><member><name>dbg</name><value>create_and_attach</value></member><member><name>sr</name><value>1</value></member><member><name>device_config</name><value><struct><member><name>SRmaster</name><value>true</value></member><member><name>device</name><value>/dev/dummy</value></member><member><name>reservation_mode</name><value>thin</value></member></struct></value></member></struct></value></param></params></methodCall>
[20131218T17:44:20.856Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|lvmabs] init_lvm: Loading Vg from devices list: [/dev/dummy]
[20131218T17:44:20.856Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|mlvm] Vg.load
[20131218T17:44:20.856Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|mlvm] Label found: 
Label header:
id: LABELONE
sector: 1
crc: 2126210908
offset: 32
ty: LVM2 001

PV Header:
pvh_id: XHZVvY-fGo4-fBdW-BD3c-xAOh-F9o3-0j5s1z
pvh_device_size: 1099511627776
pvh_areas1: {offset=10551296,size=0}
pvh_areas2: {offset=4096,size=10547200}



[20131218T17:44:20.857Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|lvmabs] init_lvm: Vg loaded:
[20131218T17:44:20.857Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|lvmabs] VG_XenStorage-1 {
id = "UP3uAf-HaRn-WFlS-H1i6-8V05-CvqZ-Ytw2Le"
seqno = 1
status = ["READ", "WRITE"]
extent_size = 8192
max_lv = 0
max_pv = 0

physical_volumes {

pv0 {
id = "XHZVvY-fGo4-fBdW-BD3c-xAOh-F9o3-0j5s1z"
device = "/dev/dummy"

status = ["ALLOCATABLE"]
dev_size = 2147483648
pe_start = 20608
pe_count = 262141
}
}

logical_volumes {
}
}
# Generated by MLVM version 0.1: 

contents = "Text Format Volume Group"
version = 1

description = ""

creation_host = "<need uname!>"
creation_time = 1387388660


[20131218T17:44:20.857Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|mlvm] write_label_and_pv_header:
PV header:
pvh_id: XHZVvY-fGo4-fBdW-BD3c-xAOh-F9o3-0j5s1z
pvh_device_size: 1099511627776
pvh_areas1: {offset=10551296,size=0}
pvh_areas2: {offset=4096,size=10547200}

[20131218T17:44:20.858Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|mlvm] Writing MDA header
[20131218T17:44:20.858Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|mlvm] Writing: checksum: 1363302805
magic:  LVM2 x[5A%r0N*>
version: 1
start: 4096
size: 10485760
raw_locns:[{offset:1024,size:738,checksum:-741370077,filler:0}]

[20131218T17:44:20.858Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|lvmabs] init_lvm: Redo log initialized:
[20131218T17:44:20.858Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|lvmabs] VG_XenStorage-1 {
id = "UP3uAf-HaRn-WFlS-H1i6-8V05-CvqZ-Ytw2Le"
seqno = 2
status = ["READ", "WRITE"]
extent_size = 8192
max_lv = 0
max_pv = 0

physical_volumes {

pv0 {
id = "XHZVvY-fGo4-fBdW-BD3c-xAOh-F9o3-0j5s1z"
device = "/dev/dummy"

status = ["ALLOCATABLE"]
dev_size = 2147483648
pe_start = 20608
pe_count = 262141
}
}

logical_volumes {

mlvm_redo_log {
id = "QFZNgy-DlHv-YjiB-aHVT-B5Xa-dzNF-YYejoU"
status = ["READ", "VISIBLE"]
segment_count = 1

segment1 {
start_extent = 0
extent_count = 1

type = "striped"
stripe_count = 1	#linear

stripes = [
"pv0", 0
]
}
}
}
}
# Generated by MLVM version 0.1: 

contents = "Text Format Volume Group"
version = 1

description = ""

creation_host = "<need uname!>"
creation_time = 1387388660


[20131218T17:44:20.858Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] add_pv_id_info: Got mutex
[20131218T17:44:20.858Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] add_pv_id_info: Released mutex
[20131218T17:44:20.858Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|vhdSlave] VhdSlave.SR.attach
[20131218T17:44:20.858Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|tapdisk] Tapdisk.scan
[20131218T17:44:20.859Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|tapdisk] parse_tapdev_link: ..
[20131218T17:44:20.859Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|tapdisk] parse_tapdev_link: .
[20131218T17:44:20.859Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|attachments] attach as slave: 1
[20131218T17:44:20.859Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|attachments] Ignoring ENOENT while reading attachments file
[20131218T17:44:20.859Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|vhdsm] mode=Master
[20131218T17:44:20.859Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|vhdmaster] m_rolling_upgrade=false
[20131218T17:44:20.859Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|lvmabs] init_lvm: Loading Vg from devices list: [/dev/dummy]
[20131218T17:44:20.859Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|mlvm] Vg.load
[20131218T17:44:20.859Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|mlvm] Label found: 
Label header:
id: LABELONE
sector: 1
crc: 2126210908
offset: 32
ty: LVM2 001

PV Header:
pvh_id: XHZVvY-fGo4-fBdW-BD3c-xAOh-F9o3-0j5s1z
pvh_device_size: 1099511627776
pvh_areas1: {offset=10551296,size=0}
pvh_areas2: {offset=4096,size=10547200}



[20131218T17:44:20.860Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|mlvm] Allocations for lv mlvm_redo_log:
(pv0: [0,1])

[20131218T17:44:20.860Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|mlvm] Redo.read
[20131218T17:44:20.860Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|mlvm] start_ofs: 12 end_ofs: 12 size: 0
[20131218T17:44:20.860Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|mlvm] Reading from pos: 0
[20131218T17:44:20.860Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|mlvm] Redo.read finished
[20131218T17:44:20.860Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|lvmabs] init_lvm: Vg loaded:
[20131218T17:44:20.860Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|lvmabs] VG_XenStorage-1 {
id = "UP3uAf-HaRn-WFlS-H1i6-8V05-CvqZ-Ytw2Le"
seqno = 2
status = ["READ", "WRITE"]
extent_size = 8192
max_lv = 0
max_pv = 0

physical_volumes {

pv0 {
id = "XHZVvY-fGo4-fBdW-BD3c-xAOh-F9o3-0j5s1z"
device = "/dev/dummy"

status = ["ALLOCATABLE"]
dev_size = 2147483648
pe_start = 20608
pe_count = 262141
}
}

logical_volumes {

mlvm_redo_log {
id = "QFZNgy-DlHv-YjiB-aHVT-B5Xa-dzNF-YYejoU"
status = ["READ", "VISIBLE"]
segment_count = 1

segment1 {
start_extent = 0
extent_count = 1

type = "striped"
stripe_count = 1	#linear

stripes = [
"pv0", 0
]
}
}
}
}
# Generated by MLVM version 0.1: 

contents = "Text Format Volume Group"
version = 1

description = ""

creation_host = "<need uname!>"
creation_time = 1387388660


[20131218T17:44:20.860Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|lvmabs] init_lvm: Redo log initialized:
[20131218T17:44:20.860Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|lvmabs] VG_XenStorage-1 {
id = "UP3uAf-HaRn-WFlS-H1i6-8V05-CvqZ-Ytw2Le"
seqno = 2
status = ["READ", "WRITE"]
extent_size = 8192
max_lv = 0
max_pv = 0

physical_volumes {

pv0 {
id = "XHZVvY-fGo4-fBdW-BD3c-xAOh-F9o3-0j5s1z"
device = "/dev/dummy"

status = ["ALLOCATABLE"]
dev_size = 2147483648
pe_start = 20608
pe_count = 262141
}
}

logical_volumes {

mlvm_redo_log {
id = "QFZNgy-DlHv-YjiB-aHVT-B5Xa-dzNF-YYejoU"
status = ["READ", "VISIBLE"]
segment_count = 1

segment1 {
start_extent = 0
extent_count = 1

type = "striped"
stripe_count = 1	#linear

stripes = [
"pv0", 0
]
}
}
}
}
# Generated by MLVM version 0.1: 

contents = "Text Format Volume Group"
version = 1

description = ""

creation_host = "<need uname!>"
creation_time = 1387388660


[20131218T17:44:20.860Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|vhdmaster] container initialised
[20131218T17:44:20.860Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|scan] scan
[20131218T17:44:20.860Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|lvmabs] operating on vg: VG_XenStorage-1 lv: mlvm_redo_log
[20131218T17:44:20.860Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|lvmabs] operating on vg: VG_XenStorage-1 lv: mlvm_redo_log
[20131218T17:44:20.860Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|mlvm] Using dm_name=41200ce1-f512-4bb1-8c35-66071cb1cbc6 (use_tmp=true)
[20131218T17:44:20.861Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|scan] Reading VHD: /tmp/dummytest/1/dev/mapper/41200ce1-f512-4bb1-8c35-66071cb1cbc6
[20131218T17:44:20.861Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|mlvm] LVM REDO: 000000000113„•¦¾   ]      -   $ B 0host_attachments 	&3T0Zmh-m0Ms-j5Hc-lAAb-yAWZ-BMfT-mMfUe8  #pv0 _j        _j        @
[20131218T17:44:20.861Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|lvmabs] operating on vg: VG_XenStorage-1 lv: host_attachments
[20131218T17:44:20.861Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|mlvm] Using dm_name=7ac8a978-eb8b-406e-9939-955b3626bd7c (use_tmp=true)
[20131218T17:44:20.861Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|lvmabs] operating on vg: VG_XenStorage-1 lv: host_attachments
[20131218T17:44:20.861Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] attach_lv: Got the mutex
[20131218T17:44:20.861Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] LV VG_XenStorage--1-host_attachments not attached: attaching. refcount now 1
[20131218T17:44:20.861Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|mlvm] Using dm_name=VG_XenStorage--1-host_attachments (use_tmp=false)
[20131218T17:44:20.861Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] attach_lv: released the mutex
[20131218T17:44:20.862Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] Remove LV: refcount for dm=VG_XenStorage--1-host_attachments is now 0
[20131218T17:44:20.862Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] Removing LV=VG_XenStorage--1-host_attachments
[20131218T17:44:20.862Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|mlvm] LVM REDO: 000000000115„•¦¾   _      -   $ C 2id_to_leaf_mapping 	&kI4sAx-JJcU-YGEi-84Cx-BnYh-qXu3-Nsfyjo  #pv0 _j        _j        @
[20131218T17:44:20.862Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|lvmabs] operating on vg: VG_XenStorage-1 lv: id_to_leaf_mapping
[20131218T17:44:20.862Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|mlvm] Using dm_name=82cacca6-7187-4ab5-b1a6-5862231d2f64 (use_tmp=true)
[20131218T17:44:20.863Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|lvmabs] operating on vg: VG_XenStorage-1 lv: id_to_leaf_mapping
[20131218T17:44:20.863Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] attach_lv: Got the mutex
[20131218T17:44:20.863Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] LV VG_XenStorage--1-id_to_leaf_mapping not attached: attaching. refcount now 1
[20131218T17:44:20.863Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|mlvm] Using dm_name=VG_XenStorage--1-id_to_leaf_mapping (use_tmp=false)
[20131218T17:44:20.863Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] attach_lv: released the mutex
[20131218T17:44:20.863Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] Remove LV: refcount for dm=VG_XenStorage--1-id_to_leaf_mapping is now 0
[20131218T17:44:20.863Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] Removing LV=VG_XenStorage--1-id_to_leaf_mapping
[20131218T17:44:20.863Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|vhdmaster] Selected provisioning policy: Thin
[20131218T17:44:20.863Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|lvmabs] operating on vg: VG_XenStorage-1 lv: host_attachments
[20131218T17:44:20.863Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] attach_lv: Got the mutex
[20131218T17:44:20.863Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] LV VG_XenStorage--1-host_attachments not attached: attaching. refcount now 1
[20131218T17:44:20.863Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|mlvm] Using dm_name=VG_XenStorage--1-host_attachments (use_tmp=false)
[20131218T17:44:20.863Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] attach_lv: released the mutex
[20131218T17:44:20.863Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] Remove LV: refcount for dm=VG_XenStorage--1-host_attachments is now 0
[20131218T17:44:20.863Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] Removing LV=VG_XenStorage--1-host_attachments
[20131218T17:44:20.863Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|vhdmaster] Recovering any slaves
[20131218T17:44:20.864Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|nmutex] wait: reason=Retrieving the hashtbl of attached hosts
[20131218T17:44:20.864Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|vhdmaster] About to get_localhost()
[20131218T17:44:20.864Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|vhdmaster] Creating the attach_part_two thread
[20131218T17:44:20.864Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|vhdmaster] About to iterate over attached slaves
[20131218T17:44:20.864Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|15||vhdmaster] attach_part_two thread created
[20131218T17:44:20.864Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|vhdSlave] Registering with master
[20131218T17:44:20.864Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|nmutex] wait: reason=Finding all attached/activated VDIs
[20131218T17:44:20.864Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|15||nmutex] wait: reason=Checking whether resync is required
[20131218T17:44:20.864Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|vhdmaster] Attach from host: 1, ip: 127.0.0.1
[20131218T17:44:20.864Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|15||vhdmaster] Attach part two
[20131218T17:44:20.864Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|slave_sr_attachments] Slave SR attach
[20131218T17:44:20.864Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|15||nmutex] wait: reason=Getting all the leaf infos
[20131218T17:44:20.864Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|nmutex] wait: reason=Adding host to the attached_hosts
[20131218T17:44:20.864Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|15||vhdmaster] Resolving map inconsistencies
[20131218T17:44:20.864Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|lvmabs] operating on vg: VG_XenStorage-1 lv: host_attachments
[20131218T17:44:20.864Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|15||vhdmaster] Resolved
[20131218T17:44:20.864Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] attach_lv: Got the mutex
[20131218T17:44:20.864Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|15||nmutex] wait: reason=setting coalesce_in_progress flag
[20131218T17:44:20.865Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] LV VG_XenStorage--1-host_attachments not attached: attaching. refcount now 1
[20131218T17:44:20.865Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|15||nmutex] wait: reason=Unsetting coalesce_in_progress flag
[20131218T17:44:20.865Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|mlvm] Using dm_name=VG_XenStorage--1-host_attachments (use_tmp=false)
[20131218T17:44:20.865Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|15||nmutex] About to broadcast
[20131218T17:44:20.865Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] attach_lv: released the mutex
[20131218T17:44:20.865Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|15||nmutex] Done
[20131218T17:44:20.865Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] Remove LV: refcount for dm=VG_XenStorage--1-host_attachments is now 0
[20131218T17:44:20.865Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] Removing LV=VG_XenStorage--1-host_attachments
[20131218T17:44:20.865Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|nmutex] wait: reason=Resyncing VDI attachments/activations
[20131218T17:44:20.865Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|vhdSlave] Registration functions finished. Setting s_ready=true
[20131218T17:44:20.865Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|vhdsm] s_ready for the slave is: true
[20131218T17:44:20.865Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|lvmabs] operating on vg: VG_XenStorage-1 lv: host_attachments
[20131218T17:44:20.865Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] attach_lv: Got the mutex
[20131218T17:44:20.865Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] LV VG_XenStorage--1-host_attachments not attached: attaching. refcount now 1
[20131218T17:44:20.866Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|mlvm] Using dm_name=VG_XenStorage--1-host_attachments (use_tmp=false)
[20131218T17:44:20.866Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] attach_lv: released the mutex
[20131218T17:44:20.866Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] Remove LV: refcount for dm=VG_XenStorage--1-host_attachments is now 0
[20131218T17:44:20.866Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|host] Removing LV=VG_XenStorage--1-host_attachments
[20131218T17:44:20.866Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|103c21f8-33f6-483e-b7b0-954768ebaaee|vhdd] Response: (omitted)
[20131218T17:44:20.879Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|16 inet-rpc||http] Request { frame = false; method = POST; uri = /internal; query = [  ]; content_length = [ 55 ]; transfer encoding = ; version = 1.0; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = ; user_agent = xen-api-libs/1.0 }
[20131218T17:44:20.879Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|16 inet-rpc||vhdd] Internal handler
[20131218T17:44:20.879Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|16 inet-rpc||vhdd] Call={"method": "SR.mode", "params": [{"sr": "1"}], "id": 1}
[20131218T17:44:20.879Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|16 inet-rpc||vhdd] Response: {"result": "Master", "error": null, "id": 0}
[20131218T17:44:20.883Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||http] Request { frame = false; method = POST; uri = /internal; query = [  ]; content_length = [ 137 ]; transfer encoding = ; version = 1.0; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = ; user_agent = xen-api-libs/1.0 }
[20131218T17:44:20.884Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||vhdd] Internal handler
[20131218T17:44:20.884Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||vhdd] Call={"method": "SR.slave_attach", "params": [{"sr": "1", "host": {"h_uuid": "2", "h_ip": "127.0.0.1", "h_port": 4095}, "vdis": {}}], "id": 2}
[20131218T17:44:20.884Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||vhdmaster] Attach from host: 2, ip: 127.0.0.1
[20131218T17:44:20.884Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||slave_sr_attachments] Slave SR attach
[20131218T17:44:20.884Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||nmutex] wait: reason=Adding host to the attached_hosts
[20131218T17:44:20.884Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||lvmabs] operating on vg: VG_XenStorage-1 lv: host_attachments
[20131218T17:44:20.884Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||host] attach_lv: Got the mutex
[20131218T17:44:20.884Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||host] LV VG_XenStorage--1-host_attachments not attached: attaching. refcount now 1
[20131218T17:44:20.884Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||mlvm] Using dm_name=VG_XenStorage--1-host_attachments (use_tmp=false)
[20131218T17:44:20.884Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||host] attach_lv: released the mutex
[20131218T17:44:20.885Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||host] Remove LV: refcount for dm=VG_XenStorage--1-host_attachments is now 0
[20131218T17:44:20.885Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||host] Removing LV=VG_XenStorage--1-host_attachments
[20131218T17:44:20.885Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||nmutex] wait: reason=Resyncing VDI attachments/activations
[20131218T17:44:20.885Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||vhdd] Response: {"result": "OK", "error": null, "id": 0}
[20131218T17:44:20.888Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 1207 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:20.888Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:20.888Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.create</methodName><params><param><value><struct><member><name>dbg</name><value>create_vdi</value></member><member><name>sr</name><value>1</value></member><member><name>vdi_info</name><value><struct><member><name>vdi</name><value>vdi</value></member><member><name>content_id</name><value></value></member><member><name>name_label</name><value>ftest_vdi</value></member><member><name>name_description</name><value></value></member><member><name>ty</name><value>user</value></member><member><name>metadata_of_pool</name><value></value></member><member><name>is_a_snapshot</name><value><boolean>0</boolean></value></member><member><name>snapshot_time</name><value></value></member><member><name>snapshot_of</name><value></value></member><member><name>read_only</name><value><boolean>0</boolean></value></member><member><name>virtual_size</name><value>52428800</value></member><member><name>physical_utilisation</name><value>0</value></member><member><name>persistent</name><value><boolean>1</boolean></value></member><member><name>sm_config</name><value><struct></struct></value></member></struct></value></member></struct></value></param></params></methodCall>
[20131218T17:44:20.888Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|vhdsm] API call: VDI.create sr=1 size=52428800 sm_config=[]
[20131218T17:44:20.888Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|vhdutil] Dump size: overhead=4312576 phys_size=4312576 virtual_size=52428800 critical_size=109170176
[20131218T17:44:20.888Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|vhdutil] leaf_status: Some false reservation_type: Thin
[20131218T17:44:20.888Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-405c2a54-dd17-48a1-b3f5-b1eddb03e0b6
[20131218T17:44:20.888Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|mlvm] LVM REDO: 000000000138„•¦¾   v      3   ' D 	(VHD-405c2a54-dd17-48a1-b3f5-b1eddb03e0b6 	&2fLxck-y7Hk-M8sY-CdXT-56Vq-8jR2-312OXW  #pv0 _j        _j        @
[20131218T17:44:20.889Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-405c2a54-dd17-48a1-b3f5-b1eddb03e0b6
[20131218T17:44:20.889Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|host] attach_lv: Got the mutex
[20131218T17:44:20.889Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|host] LV VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6 not attached: attaching. refcount now 1
[20131218T17:44:20.889Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|mlvm] Using dm_name=VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6 (use_tmp=false)
[20131218T17:44:20.889Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|host] attach_lv: released the mutex
[20131218T17:44:20.945Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|vhdutil] query_size_vhd: Querying size of VHD a5ea844f-dfa1-4e24-bbe3-d3907f2657fc
[20131218T17:44:20.945Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|vhdutil] query_size_vhd: get_phys_size returned 4311040
[20131218T17:44:20.946Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|vhdutil] query_size_vhd: first_allocated_block=None
[20131218T17:44:20.946Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|vhdutil] query_size_vhd: virtual_size=52428800
[20131218T17:44:20.946Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|nmutex] wait: reason=Adding VHD uid='a5ea844f-dfa1-4e24-bbe3-d3907f2657fc'
[20131218T17:44:20.946Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6 is now 0
[20131218T17:44:20.946Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|host] Removing LV=VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6
[20131218T17:44:20.946Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|nmutex] wait: reason=Adding a new ID to the mapping (2ab414f7-90b6-476f-a29d-6e51d3cf2c7e->PVhd 'a5ea844f-dfa1-4e24-bbe3-d3907f2657fc')
[20131218T17:44:20.946Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|lvmabs] operating on vg: VG_XenStorage-1 lv: id_to_leaf_mapping
[20131218T17:44:20.946Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|host] attach_lv: Got the mutex
[20131218T17:44:20.946Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|host] LV VG_XenStorage--1-id_to_leaf_mapping not attached: attaching. refcount now 1
[20131218T17:44:20.946Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|mlvm] Using dm_name=VG_XenStorage--1-id_to_leaf_mapping (use_tmp=false)
[20131218T17:44:20.946Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|host] attach_lv: released the mutex
[20131218T17:44:20.947Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|host] Remove LV: refcount for dm=VG_XenStorage--1-id_to_leaf_mapping is now 0
[20131218T17:44:20.947Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|host] Removing LV=VG_XenStorage--1-id_to_leaf_mapping
[20131218T17:44:20.947Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|dd0224cd-01ec-449f-b90e-1ca5995f1141|vhdd] Response: (omitted)
[20131218T17:44:20.948Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 486 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:20.948Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:20.949Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.attach</methodName><params><param><value><struct><member><name>dbg</name><value>dbg</value></member><member><name>dp</name><value>2ab414f7-90b6-476f-a29d-6e51d3cf2c7e</value></member><member><name>sr</name><value>1</value></member><member><name>vdi</name><value>2ab414f7-90b6-476f-a29d-6e51d3cf2c7e</value></member><member><name>read_write</name><value><boolean>1</boolean></value></member></struct></value></param></params></methodCall>
[20131218T17:44:20.949Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|vhdsm] API call: VDI.attach sr=1 vdi=2ab414f7-90b6-476f-a29d-6e51d3cf2c7e writable=true
[20131218T17:44:20.949Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:20.949Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|nmutex] wait: reason=Adding to current_operations
[20131218T17:44:20.949Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|vhdSlave] Checking current ops
[20131218T17:44:20.949Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:20.949Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|nmutex] wait: reason=Finding whether we're already attached
[20131218T17:44:20.949Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|nmutex] wait: reason=Retrieving the hashtbl of attached hosts
[20131218T17:44:20.949Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|vhdmaster] Got to the slave_attach function call
[20131218T17:44:20.949Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:20.949Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|nmutex] wait: reason=Executing with operation '"OpAttaching"'
[20131218T17:44:20.949Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:20.949Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|nmutex] wait: reason=Getting VHD uid='a5ea844f-dfa1-4e24-bbe3-d3907f2657fc'
[20131218T17:44:20.949Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|vhdMaster_utils] Resizing VHD uid: a5ea844f-dfa1-4e24-bbe3-d3907f2657fc
[20131218T17:44:20.949Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-405c2a54-dd17-48a1-b3f5-b1eddb03e0b6
[20131218T17:44:20.949Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|mlvm] Using dm_name=66cedf55-eeaf-432f-be3b-81fe44426016 (use_tmp=true)
[20131218T17:44:20.950Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|vhdutil] Dump size: overhead=4311040 phys_size=4311040 virtual_size=52428800 critical_size=56739840
[20131218T17:44:20.950Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|vhdutil] leaf_status: Some true reservation_type: Thin
[20131218T17:44:20.950Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-405c2a54-dd17-48a1-b3f5-b1eddb03e0b6
[20131218T17:44:20.950Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|vhdMaster_utils] new_size=old_size=58720256. Not doing anything
[20131218T17:44:20.950Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|nmutex] wait: reason=Update id_map
[20131218T17:44:20.950Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|locking] update_leaf: vdi_location=2ab414f7-90b6-476f-a29d-6e51d3cf2c7e
[20131218T17:44:20.950Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|nmutex] wait: reason=Finished op '"OpAttaching"'. Removing from cur
[20131218T17:44:20.950Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|nmutex] About to broadcast
[20131218T17:44:20.950Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|nmutex] Done
[20131218T17:44:20.950Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|nmutex] wait: reason=Getting a copy of a VHD chain
[20131218T17:44:20.950Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-405c2a54-dd17-48a1-b3f5-b1eddb03e0b6
[20131218T17:44:20.950Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-405c2a54-dd17-48a1-b3f5-b1eddb03e0b6
[20131218T17:44:20.950Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-405c2a54-dd17-48a1-b3f5-b1eddb03e0b6
[20131218T17:44:20.950Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|vhdSlave] Got response: leaf=/tmp/dummytest/1//dev/mapper/VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6
[20131218T17:44:20.950Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:20.950Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|nmutex] wait: reason=Adding to master_approved_ops
[20131218T17:44:20.950Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|vhdSlave] Checking current ops
[20131218T17:44:20.950Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:20.951Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|vhdSlave] LV name: VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6
[20131218T17:44:20.951Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|host] attach_lv: Got the mutex
[20131218T17:44:20.951Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|host] LV VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6 not attached: attaching. refcount now 1
[20131218T17:44:20.951Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|mlvm] Using dm_name=VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6 (use_tmp=false)
[20131218T17:44:20.951Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|host] attach_lv: released the mutex
[20131218T17:44:20.952Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|vhdSlave] s_mutex lock: attach_from_sai
[20131218T17:44:20.952Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|nmutex] wait: reason=Adding info to s_attached_vdis
[20131218T17:44:20.952Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:20.952Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|nmutex] wait: reason=Removing from master_approved_ops
[20131218T17:44:20.952Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|vhdSlave] Removing my current op
[20131218T17:44:20.952Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:20.952Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|nmutex] About to broadcast
[20131218T17:44:20.952Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|nmutex] Done
[20131218T17:44:20.952Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:20.952Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|nmutex] wait: reason=Removing from current_operations
[20131218T17:44:20.952Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|vhdSlave] Removing my current op
[20131218T17:44:20.953Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:20.953Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|nmutex] About to broadcast
[20131218T17:44:20.953Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|nmutex] Done
[20131218T17:44:20.953Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|a2d555bc-f3fc-4dde-84d8-e37e6e731303|vhdd] Response: (omitted)
[20131218T17:44:20.954Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 413 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:20.954Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:20.954Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.activate</methodName><params><param><value><struct><member><name>dbg</name><value>dbg</value></member><member><name>dp</name><value>2ab414f7-90b6-476f-a29d-6e51d3cf2c7e</value></member><member><name>sr</name><value>1</value></member><member><name>vdi</name><value>2ab414f7-90b6-476f-a29d-6e51d3cf2c7e</value></member></struct></value></param></params></methodCall>
[20131218T17:44:20.954Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|vhdsm] API call: VDI.activate sr=1 vdi=2ab414f7-90b6-476f-a29d-6e51d3cf2c7e
[20131218T17:44:20.955Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:20.955Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|nmutex] wait: reason=Adding to current_operations
[20131218T17:44:20.955Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|vhdSlave] Checking current ops
[20131218T17:44:20.955Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:20.955Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|nmutex] wait: reason=Checking whether the VDI is activated
[20131218T17:44:20.955Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:20.955Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|nmutex] wait: reason=Executing with operation '"OpActivating"'
[20131218T17:44:20.955Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:20.955Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|nmutex] wait: reason=Update id_map
[20131218T17:44:20.955Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|locking] update_leaf: vdi_location=2ab414f7-90b6-476f-a29d-6e51d3cf2c7e
[20131218T17:44:20.955Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|nmutex] wait: reason=Finished op '"OpActivating"'. Removing from cur
[20131218T17:44:20.955Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|nmutex] About to broadcast
[20131218T17:44:20.955Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|nmutex] Done
[20131218T17:44:20.955Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:20.955Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|nmutex] wait: reason=Adding to master_approved_ops
[20131218T17:44:20.955Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|vhdSlave] Checking current ops
[20131218T17:44:20.955Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:20.955Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|nmutex] wait: reason=Activating tapdisk
[20131218T17:44:20.956Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|tapdisk_listen] Registered to listen to /dev/shm/1_1_2ab414f7-90b6-476f-a29d-6e51d3cf2c7e.stats
[20131218T17:44:20.956Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|vhdSlave] Setting maxsize in shared page
[20131218T17:44:20.956Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|vhdSlave] Setting maxsize=58720256
[20131218T17:44:20.956Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:20.956Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|nmutex] wait: reason=Removing from master_approved_ops
[20131218T17:44:20.956Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|vhdSlave] Removing my current op
[20131218T17:44:20.956Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:20.956Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|nmutex] About to broadcast
[20131218T17:44:20.956Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|nmutex] Done
[20131218T17:44:20.956Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:20.957Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|nmutex] wait: reason=Removing from current_operations
[20131218T17:44:20.957Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|vhdSlave] Removing my current op
[20131218T17:44:20.957Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:20.957Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|nmutex] About to broadcast
[20131218T17:44:20.957Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|nmutex] Done
[20131218T17:44:20.957Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|5d5feeb6-abf7-4c0d-97e4-61271785edfb|vhdd] Response: (omitted)
[20131218T17:44:20.960Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|21 inet-rpc||http] Request { frame = false; method = POST; uri = /internal; query = [  ]; content_length = [ 155 ]; transfer encoding = ; version = 1.0; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = ; user_agent = xen-api-libs/1.0 }
[20131218T17:44:20.960Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|21 inet-rpc||vhdd] Internal handler
[20131218T17:44:20.960Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|21 inet-rpc||vhdd] Call={"method": "Debug.write_junk", "params": [{"sr": "1", "vdi": "2ab414f7-90b6-476f-a29d-6e51d3cf2c7e", "size": 41943040, "n": 10, "current": []}], "id": 929}
[20131218T17:44:20.960Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|21 inet-rpc||vhdsm] Writing junk
[20131218T17:44:20.960Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|21 inet-rpc||vhdd] Response: {"result": [], "error": null, "id": 0}
[20131218T17:44:20.961Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|22 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 326 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:20.961Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|22 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:20.961Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|22 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.stat</methodName><params><param><value><struct><member><name>dbg</name><value>dbg</value></member><member><name>sr</name><value>1</value></member><member><name>vdi</name><value>2ab414f7-90b6-476f-a29d-6e51d3cf2c7e</value></member></struct></value></param></params></methodCall>
[20131218T17:44:20.961Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|22 inet-rpc|e0bbe674-e7db-4038-9667-76f143e32f55|vhdsm] API call: VDI.stat
[20131218T17:44:20.961Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|22 inet-rpc|e0bbe674-e7db-4038-9667-76f143e32f55|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:20.961Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|22 inet-rpc|e0bbe674-e7db-4038-9667-76f143e32f55|nmutex] wait: reason=Getting VHD uid='a5ea844f-dfa1-4e24-bbe3-d3907f2657fc'
[20131218T17:44:20.961Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|22 inet-rpc|e0bbe674-e7db-4038-9667-76f143e32f55|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-405c2a54-dd17-48a1-b3f5-b1eddb03e0b6
[20131218T17:44:20.962Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|22 inet-rpc|e0bbe674-e7db-4038-9667-76f143e32f55|vhdd] Response: (omitted)
[20131218T17:44:20.963Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 1260 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:20.963Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:20.963Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.snapshot</methodName><params><param><value><struct><member><name>dbg</name><value>dbg</value></member><member><name>sr</name><value>1</value></member><member><name>vdi_info</name><value><struct><member><name>vdi</name><value>2ab414f7-90b6-476f-a29d-6e51d3cf2c7e</value></member><member><name>content_id</name><value></value></member><member><name>name_label</name><value>ftest_vdi</value></member><member><name>name_description</name><value></value></member><member><name>ty</name><value>user</value></member><member><name>metadata_of_pool</name><value></value></member><member><name>is_a_snapshot</name><value><boolean>0</boolean></value></member><member><name>snapshot_time</name><value>19700101T00:00:00Z</value></member><member><name>snapshot_of</name><value></value></member><member><name>read_only</name><value><boolean>0</boolean></value></member><member><name>virtual_size</name><value>52428800</value></member><member><name>physical_utilisation</name><value>58720256</value></member><member><name>persistent</name><value><boolean>1</boolean></value></member><member><name>sm_config</name><value><struct></struct></value></member></struct></value></member></struct></value></param></params></methodCall>
[20131218T17:44:20.964Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdsm] API call: VDI.snapshot sr=1 vdi=2ab414f7-90b6-476f-a29d-6e51d3cf2c7e
[20131218T17:44:20.964Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdmaster] Clone: Checking all hosts present
[20131218T17:44:20.964Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Retrieving the hashtbl of attached hosts
[20131218T17:44:20.964Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdmaster] OK
[20131218T17:44:20.964Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:20.964Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Executing with operation '"OpClone"'
[20131218T17:44:20.964Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:20.964Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Getting VHD uid='a5ea844f-dfa1-4e24-bbe3-d3907f2657fc'
[20131218T17:44:20.964Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|clone] Leaf clone: creating new leaf vhd for original VDI
[20131218T17:44:20.964Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdutil] Dump size: overhead=4312576 phys_size=4312576 virtual_size=52428800 critical_size=109170176
[20131218T17:44:20.964Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdutil] leaf_status: Some true reservation_type: Thin
[20131218T17:44:20.964Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdMaster_utils] Create child: maybe about to create a LV, size=58720256
[20131218T17:44:20.965Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-deec8f12-e51b-4297-8bda-4963a47dc8f5
[20131218T17:44:20.965Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|mlvm] LVM REDO: 000000000138„•¦¾   v      3   ' E 	(VHD-deec8f12-e51b-4297-8bda-4963a47dc8f5 	&vNpwdU-eaD9-zqn5-5r8L-AyEd-ixcS-udpJdb  #pv0 _j        _j        @
[20131218T17:44:20.965Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdMaster_utils] Created LVM volume with uuid=deec8f12-e51b-4297-8bda-4963a47dc8f5
[20131218T17:44:20.965Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-405c2a54-dd17-48a1-b3f5-b1eddb03e0b6
[20131218T17:44:20.965Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] attach_lv: Got the mutex
[20131218T17:44:20.965Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] LV VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6 already attached
[20131218T17:44:20.965Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] attach_lv: increasing refcount for dm=VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6 to 2
[20131218T17:44:20.965Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] attach_lv: released the mutex
[20131218T17:44:20.965Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-deec8f12-e51b-4297-8bda-4963a47dc8f5
[20131218T17:44:20.965Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] attach_lv: Got the mutex
[20131218T17:44:20.965Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] LV VG_XenStorage--1-VHD--deec8f12--e51b--4297--8bda--4963a47dc8f5 not attached: attaching. refcount now 1
[20131218T17:44:20.965Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|mlvm] Using dm_name=VG_XenStorage--1-VHD--deec8f12--e51b--4297--8bda--4963a47dc8f5 (use_tmp=false)
[20131218T17:44:20.965Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] attach_lv: released the mutex
[20131218T17:44:21.000Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdutil] query_size_vhd: Querying size of VHD 068ef7a5-2553-4065-8ace-e35267f66f1c
[20131218T17:44:21.000Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdutil] query_size_vhd: get_phys_size returned 4312576
[20131218T17:44:21.000Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdutil] query_size_vhd: first_allocated_block=None
[20131218T17:44:21.000Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdutil] query_size_vhd: virtual_size=52428800
[20131218T17:44:21.004Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--deec8f12--e51b--4297--8bda--4963a47dc8f5 is now 0
[20131218T17:44:21.004Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] Removing LV=VG_XenStorage--1-VHD--deec8f12--e51b--4297--8bda--4963a47dc8f5
[20131218T17:44:21.005Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6 is now 1
[20131218T17:44:21.005Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Adding VHD uid='068ef7a5-2553-4065-8ace-e35267f66f1c'
[20131218T17:44:21.005Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdMaster_utils] New VHD inserted into Hashtbl uid=068ef7a5-2553-4065-8ace-e35267f66f1c
[20131218T17:44:21.005Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Remapping an ID (2ab414f7-90b6-476f-a29d-6e51d3cf2c7e->PVhd '068ef7a5-2553-4065-8ace-e35267f66f1c')
[20131218T17:44:21.005Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: id_to_leaf_mapping
[20131218T17:44:21.005Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] attach_lv: Got the mutex
[20131218T17:44:21.005Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] LV VG_XenStorage--1-id_to_leaf_mapping not attached: attaching. refcount now 1
[20131218T17:44:21.005Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|mlvm] Using dm_name=VG_XenStorage--1-id_to_leaf_mapping (use_tmp=false)
[20131218T17:44:21.005Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] attach_lv: released the mutex
[20131218T17:44:21.005Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] Remove LV: refcount for dm=VG_XenStorage--1-id_to_leaf_mapping is now 0
[20131218T17:44:21.005Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] Removing LV=VG_XenStorage--1-id_to_leaf_mapping
[20131218T17:44:21.006Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdMaster_utils] Calling reattach
[20131218T17:44:21.006Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:21.006Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Executing with operation '"OpReattaching"'
[20131218T17:44:21.006Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:21.006Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Getting VHD uid='068ef7a5-2553-4065-8ace-e35267f66f1c'
[20131218T17:44:21.006Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdMaster_utils] Resizing VHD uid: 068ef7a5-2553-4065-8ace-e35267f66f1c
[20131218T17:44:21.006Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-deec8f12-e51b-4297-8bda-4963a47dc8f5
[20131218T17:44:21.006Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|mlvm] Using dm_name=36f6e0d3-b178-4fda-9273-ff1fa6cc7b79 (use_tmp=true)
[20131218T17:44:21.006Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdutil] Dump size: overhead=4312576 phys_size=4312576 virtual_size=52428800 critical_size=56741376
[20131218T17:44:21.006Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdutil] leaf_status: Some true reservation_type: Thin
[20131218T17:44:21.006Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-deec8f12-e51b-4297-8bda-4963a47dc8f5
[20131218T17:44:21.006Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdMaster_utils] new_size=old_size=58720256. Not doing anything
[20131218T17:44:21.006Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Getting a copy of a VHD chain
[20131218T17:44:21.006Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-deec8f12-e51b-4297-8bda-4963a47dc8f5
[20131218T17:44:21.007Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-deec8f12-e51b-4297-8bda-4963a47dc8f5
[20131218T17:44:21.007Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-deec8f12-e51b-4297-8bda-4963a47dc8f5
[20131218T17:44:21.007Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-405c2a54-dd17-48a1-b3f5-b1eddb03e0b6
[20131218T17:44:21.007Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Finished op '"OpReattaching"'. Removing from cur
[20131218T17:44:21.007Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] About to broadcast
[20131218T17:44:21.007Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] Done
[20131218T17:44:21.007Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdMaster_utils] Reattaching host: 1
[20131218T17:44:21.007Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Find the host's IP
[20131218T17:44:21.007Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Retrieving the hashtbl of attached hosts
[20131218T17:44:21.007Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdSlave] Got slave_reload call on the following VDIs:
[20131218T17:44:21.007Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdSlave] id: 2ab414f7-90b6-476f-a29d-6e51d3cf2c7e
[20131218T17:44:21.007Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:21.007Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Adding to master_approved_ops
[20131218T17:44:21.007Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdSlave] Checking current ops
[20131218T17:44:21.007Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:21.007Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Making sure we're attached
[20131218T17:44:21.008Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Performing inner part of reattach
[20131218T17:44:21.008Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] attach_lv: Got the mutex
[20131218T17:44:21.008Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] LV VG_XenStorage--1-VHD--deec8f12--e51b--4297--8bda--4963a47dc8f5 not attached: attaching. refcount now 1
[20131218T17:44:21.008Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|mlvm] Using dm_name=VG_XenStorage--1-VHD--deec8f12--e51b--4297--8bda--4963a47dc8f5 (use_tmp=false)
[20131218T17:44:21.008Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] attach_lv: released the mutex
[20131218T17:44:21.008Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdSlave] Pausing tapdisk...
[20131218T17:44:21.009Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:21.009Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Removing from master_approved_ops
[20131218T17:44:21.009Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdSlave] Removing my current op
[20131218T17:44:21.009Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:21.009Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] About to broadcast
[20131218T17:44:21.009Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] Done
[20131218T17:44:21.009Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|clone] Creating new leaf for new VDI
[20131218T17:44:21.009Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdutil] Dump size: overhead=4312576 phys_size=4312576 virtual_size=52428800 critical_size=109170176
[20131218T17:44:21.009Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdutil] leaf_status: Some false reservation_type: Attach
[20131218T17:44:21.009Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdMaster_utils] Create child: maybe about to create a LV, size=8388608
[20131218T17:44:21.009Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-8653300a-bbd4-4332-b708-b8eadc6bd5e2
[20131218T17:44:21.009Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|mlvm] LVM REDO: 000000000138„•¦¾   v      3   ' F 	(VHD-8653300a-bbd4-4332-b708-b8eadc6bd5e2 	&ecgIWR-rxKw-DaQX-jnNF-gy69-udpw-8CEuaK  #pv0 _j        _j        @
[20131218T17:44:21.010Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdMaster_utils] Created LVM volume with uuid=8653300a-bbd4-4332-b708-b8eadc6bd5e2
[20131218T17:44:21.010Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-405c2a54-dd17-48a1-b3f5-b1eddb03e0b6
[20131218T17:44:21.010Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] attach_lv: Got the mutex
[20131218T17:44:21.010Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] LV VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6 already attached
[20131218T17:44:21.010Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] attach_lv: increasing refcount for dm=VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6 to 2
[20131218T17:44:21.010Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] attach_lv: released the mutex
[20131218T17:44:21.010Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-8653300a-bbd4-4332-b708-b8eadc6bd5e2
[20131218T17:44:21.010Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] attach_lv: Got the mutex
[20131218T17:44:21.010Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] LV VG_XenStorage--1-VHD--8653300a--bbd4--4332--b708--b8eadc6bd5e2 not attached: attaching. refcount now 1
[20131218T17:44:21.010Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|mlvm] Using dm_name=VG_XenStorage--1-VHD--8653300a--bbd4--4332--b708--b8eadc6bd5e2 (use_tmp=false)
[20131218T17:44:21.010Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] attach_lv: released the mutex
[20131218T17:44:21.050Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdutil] query_size_vhd: Querying size of VHD ee8a2145-ae74-4fc4-9790-36d8dcb4d1fb
[20131218T17:44:21.051Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdutil] query_size_vhd: get_phys_size returned 4312576
[20131218T17:44:21.051Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdutil] query_size_vhd: first_allocated_block=None
[20131218T17:44:21.051Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdutil] query_size_vhd: virtual_size=52428800
[20131218T17:44:21.060Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--8653300a--bbd4--4332--b708--b8eadc6bd5e2 is now 0
[20131218T17:44:21.061Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] Removing LV=VG_XenStorage--1-VHD--8653300a--bbd4--4332--b708--b8eadc6bd5e2
[20131218T17:44:21.061Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6 is now 1
[20131218T17:44:21.061Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Adding VHD uid='ee8a2145-ae74-4fc4-9790-36d8dcb4d1fb'
[20131218T17:44:21.061Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdMaster_utils] New VHD inserted into Hashtbl uid=ee8a2145-ae74-4fc4-9790-36d8dcb4d1fb
[20131218T17:44:21.061Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Getting VHD uid='a5ea844f-dfa1-4e24-bbe3-d3907f2657fc'
[20131218T17:44:21.061Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdMaster_utils] Calling set_hidden on VHD uid=a5ea844f-dfa1-4e24-bbe3-d3907f2657fc
[20131218T17:44:21.061Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-405c2a54-dd17-48a1-b3f5-b1eddb03e0b6
[20131218T17:44:21.061Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] attach_lv: Got the mutex
[20131218T17:44:21.061Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] LV VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6 already attached
[20131218T17:44:21.061Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] attach_lv: increasing refcount for dm=VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6 to 2
[20131218T17:44:21.061Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] attach_lv: released the mutex
[20131218T17:44:21.061Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdutil] query_size_vhd: Querying size of VHD a5ea844f-dfa1-4e24-bbe3-d3907f2657fc
[20131218T17:44:21.061Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdutil] query_size_vhd: get_phys_size returned 4311040
[20131218T17:44:21.062Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdutil] query_size_vhd: first_allocated_block=None
[20131218T17:44:21.062Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdutil] query_size_vhd: virtual_size=52428800
[20131218T17:44:21.066Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6 is now 1
[20131218T17:44:21.067Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdMaster_utils] Resizing VHD uid: a5ea844f-dfa1-4e24-bbe3-d3907f2657fc
[20131218T17:44:21.067Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-405c2a54-dd17-48a1-b3f5-b1eddb03e0b6
[20131218T17:44:21.067Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|mlvm] Using dm_name=e4f3cac9-df23-40ae-b2ec-cc89ed7f32cf (use_tmp=true)
[20131218T17:44:21.070Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdutil] Dump size: overhead=4311040 phys_size=4311040 virtual_size=52428800 critical_size=56739840
[20131218T17:44:21.070Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdutil] leaf_status: None reservation_type: Thin
[20131218T17:44:21.070Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-405c2a54-dd17-48a1-b3f5-b1eddb03e0b6
[20131218T17:44:21.070Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdMaster_utils] old_size=58720256 new_size=8388608
[20131218T17:44:21.070Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-405c2a54-dd17-48a1-b3f5-b1eddb03e0b6
[20131218T17:44:21.070Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|mlvm] Beginning reduce_size_to:
[20131218T17:44:21.070Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|mlvm] Lv.reduce_size_to: s.s_start_extent=0 s.s_extent_count=14 left=2
[20131218T17:44:21.070Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|mlvm] LVM REDO: 000000000078„•¦¾   :          G¡	(VHD-405c2a54-dd17-48a1-b3f5-b1eddb03e0b6_j        
[20131218T17:44:21.071Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-405c2a54-dd17-48a1-b3f5-b1eddb03e0b6
[20131218T17:44:21.071Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-405c2a54-dd17-48a1-b3f5-b1eddb03e0b6
[20131218T17:44:21.071Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] attach_lv: Got the mutex
[20131218T17:44:21.071Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] LV info has changed - altering dm tables
[20131218T17:44:21.071Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] oldty: Mlvm (VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6,{"m": [{"start": 0, "len": 114688, "map": ["Linear", {"device": ["Dereferenced", "XHZVvY-fGo4-fBdW-BD3c-xAOh-F9o3-0j5s1z"], "offset": 45184}]}]})
[20131218T17:44:21.071Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] newty: Mlvm (VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6,{"m": [{"start": 0, "len": 16384, "map": ["Linear", {"device": ["Dereferenced", "XHZVvY-fGo4-fBdW-BD3c-xAOh-F9o3-0j5s1z"], "offset": 45184}]}]})
[20131218T17:44:21.071Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] attach_lv: increasing refcount for dm=VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6 to 2
[20131218T17:44:21.071Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] attach_lv: released the mutex
[20131218T17:44:21.078Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6 is now 1
[20131218T17:44:21.078Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdMaster_utils] Calling reattach
[20131218T17:44:21.078Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:21.078Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Executing with operation '"OpReattaching"'
[20131218T17:44:21.078Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:21.078Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Getting VHD uid='068ef7a5-2553-4065-8ace-e35267f66f1c'
[20131218T17:44:21.078Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdMaster_utils] Resizing VHD uid: 068ef7a5-2553-4065-8ace-e35267f66f1c
[20131218T17:44:21.078Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-deec8f12-e51b-4297-8bda-4963a47dc8f5
[20131218T17:44:21.078Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|mlvm] Using dm_name=a7569de1-7912-436f-a440-58efc935a91f (use_tmp=true)
[20131218T17:44:21.079Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdutil] Dump size: overhead=4312576 phys_size=4312576 virtual_size=52428800 critical_size=56741376
[20131218T17:44:21.079Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdutil] leaf_status: Some true reservation_type: Thin
[20131218T17:44:21.079Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-deec8f12-e51b-4297-8bda-4963a47dc8f5
[20131218T17:44:21.079Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdMaster_utils] new_size=old_size=58720256. Not doing anything
[20131218T17:44:21.079Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Getting a copy of a VHD chain
[20131218T17:44:21.079Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-deec8f12-e51b-4297-8bda-4963a47dc8f5
[20131218T17:44:21.079Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-deec8f12-e51b-4297-8bda-4963a47dc8f5
[20131218T17:44:21.079Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-deec8f12-e51b-4297-8bda-4963a47dc8f5
[20131218T17:44:21.079Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-405c2a54-dd17-48a1-b3f5-b1eddb03e0b6
[20131218T17:44:21.079Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Finished op '"OpReattaching"'. Removing from cur
[20131218T17:44:21.079Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] About to broadcast
[20131218T17:44:21.079Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] Done
[20131218T17:44:21.079Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdMaster_utils] Reattaching host: 1
[20131218T17:44:21.079Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Find the host's IP
[20131218T17:44:21.079Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Retrieving the hashtbl of attached hosts
[20131218T17:44:21.079Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdSlave] Got slave_reload call on the following VDIs:
[20131218T17:44:21.080Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdSlave] id: 2ab414f7-90b6-476f-a29d-6e51d3cf2c7e
[20131218T17:44:21.080Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:21.080Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Adding to master_approved_ops
[20131218T17:44:21.080Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdSlave] Checking current ops
[20131218T17:44:21.080Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:21.080Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Making sure we're attached
[20131218T17:44:21.080Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Performing inner part of reattach
[20131218T17:44:21.080Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdSlave] Pausing tapdisk...
[20131218T17:44:21.080Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] change_lv: Got the mutex
[20131218T17:44:21.081Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] change_lv: Released the mutex
[20131218T17:44:21.081Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:21.081Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Removing from master_approved_ops
[20131218T17:44:21.081Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdSlave] Removing my current op
[20131218T17:44:21.081Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:21.081Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] About to broadcast
[20131218T17:44:21.081Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] Done
[20131218T17:44:21.081Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Update_vhd_size uid='a5ea844f-dfa1-4e24-bbe3-d3907f2657fc'
[20131218T17:44:21.081Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhd_records] newsize=overhead=4311040 phys_size=4311040 virtual_size=52428800 critical_size=56739840
[20131218T17:44:21.081Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Update_hidden ptr='["PVhd", "a5ea844f-dfa1-4e24-bbe3-d3907f2657fc"]' hidden=2
[20131218T17:44:21.081Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|clone] Marked vhd: a5ea844f-dfa1-4e24-bbe3-d3907f2657fc as hidden=2
[20131218T17:44:21.081Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|coalesce] Beginning relink phase of vhd: a5ea844f-dfa1-4e24-bbe3-d3907f2657fc
[20131218T17:44:21.081Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Finding the children of parent ["PVhd", "a5ea844f-dfa1-4e24-bbe3-d3907f2657fc"]
[20131218T17:44:21.081Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Getting VHD uid='a5ea844f-dfa1-4e24-bbe3-d3907f2657fc'
[20131218T17:44:21.082Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Adding a new ID to the mapping (98f55d9a-ddbc-4c06-b519-efd6b7e257fb->PVhd 'ee8a2145-ae74-4fc4-9790-36d8dcb4d1fb')
[20131218T17:44:21.082Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|lvmabs] operating on vg: VG_XenStorage-1 lv: id_to_leaf_mapping
[20131218T17:44:21.082Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] attach_lv: Got the mutex
[20131218T17:44:21.082Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] LV VG_XenStorage--1-id_to_leaf_mapping not attached: attaching. refcount now 1
[20131218T17:44:21.082Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|mlvm] Using dm_name=VG_XenStorage--1-id_to_leaf_mapping (use_tmp=false)
[20131218T17:44:21.082Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] attach_lv: released the mutex
[20131218T17:44:21.082Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] Remove LV: refcount for dm=VG_XenStorage--1-id_to_leaf_mapping is now 0
[20131218T17:44:21.082Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|host] Removing LV=VG_XenStorage--1-id_to_leaf_mapping
[20131218T17:44:21.082Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] wait: reason=Finished op '"OpClone"'. Removing from cur
[20131218T17:44:21.082Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] About to broadcast
[20131218T17:44:21.082Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|nmutex] Done
[20131218T17:44:21.083Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|25ba166d-4156-4cb1-bc12-c33e475bee88|vhdd] Response: (omitted)
[20131218T17:44:21.084Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|24 inet-rpc||http] Request { frame = false; method = POST; uri = /internal; query = [  ]; content_length = [ 143 ]; transfer encoding = ; version = 1.0; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = ; user_agent = xen-api-libs/1.0 }
[20131218T17:44:21.084Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|24 inet-rpc||vhdd] Internal handler
[20131218T17:44:21.084Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|24 inet-rpc||vhdd] Call={"method": "Debug.check_junk", "params": [{"sr": "1", "vdi": "2ab414f7-90b6-476f-a29d-6e51d3cf2c7e", "current": [[[[0, 10]], 55]]}], "id": 930}
[20131218T17:44:21.084Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|24 inet-rpc||vhdsm] Checking junk
[20131218T17:44:21.084Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|24 inet-rpc||vhdsm] Junk OK (1)
[20131218T17:44:21.084Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|24 inet-rpc||vhdd] Response: {"result": null, "error": null, "id": 0}
[20131218T17:44:21.086Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|25 inet-rpc||http] Request { frame = false; method = POST; uri = /internal; query = [  ]; content_length = [ 128 ]; transfer encoding = ; version = 1.0; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = ; user_agent = xen-api-libs/1.0 }
[20131218T17:44:21.086Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|25 inet-rpc||vhdd] Internal handler
[20131218T17:44:21.086Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|25 inet-rpc||vhdd] Call={"method": "Debug.check_junk", "params": [{"sr": "1", "vdi": "2ab414f7-90b6-476f-a29d-6e51d3cf2c7e", "current": []}], "id": 931}
[20131218T17:44:21.086Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|25 inet-rpc||vhdsm] Checking junk
[20131218T17:44:21.086Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|25 inet-rpc||vhdsm] Junk OK (0)
[20131218T17:44:21.086Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|25 inet-rpc||vhdd] Response: {"result": null, "error": null, "id": 0}
[20131218T17:44:21.087Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 415 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:21.087Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:21.087Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.deactivate</methodName><params><param><value><struct><member><name>dbg</name><value>dbg</value></member><member><name>dp</name><value>2ab414f7-90b6-476f-a29d-6e51d3cf2c7e</value></member><member><name>sr</name><value>1</value></member><member><name>vdi</name><value>2ab414f7-90b6-476f-a29d-6e51d3cf2c7e</value></member></struct></value></param></params></methodCall>
[20131218T17:44:21.087Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|0864b9a7-4f3f-4741-970f-8fc18e8fb815|vhdsm] API call: VDI.deactivate sr=1 vdi=2ab414f7-90b6-476f-a29d-6e51d3cf2c7e
[20131218T17:44:21.088Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|0864b9a7-4f3f-4741-970f-8fc18e8fb815|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:21.088Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|0864b9a7-4f3f-4741-970f-8fc18e8fb815|nmutex] wait: reason=Adding to current_operations
[20131218T17:44:21.088Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|0864b9a7-4f3f-4741-970f-8fc18e8fb815|vhdSlave] Checking current ops
[20131218T17:44:21.088Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|0864b9a7-4f3f-4741-970f-8fc18e8fb815|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:21.088Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|0864b9a7-4f3f-4741-970f-8fc18e8fb815|nmutex] wait: reason=Deactivating
[20131218T17:44:21.088Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|0864b9a7-4f3f-4741-970f-8fc18e8fb815|tapdisk_listen] Unregistering 1/2ab414f7-90b6-476f-a29d-6e51d3cf2c7e
[20131218T17:44:21.088Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|0864b9a7-4f3f-4741-970f-8fc18e8fb815|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:21.088Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|0864b9a7-4f3f-4741-970f-8fc18e8fb815|nmutex] wait: reason=Executing with operation '"OpDeactivating"'
[20131218T17:44:21.088Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|0864b9a7-4f3f-4741-970f-8fc18e8fb815|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:21.088Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|0864b9a7-4f3f-4741-970f-8fc18e8fb815|nmutex] wait: reason=Update id_map
[20131218T17:44:21.089Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|0864b9a7-4f3f-4741-970f-8fc18e8fb815|locking] update_leaf: vdi_location=2ab414f7-90b6-476f-a29d-6e51d3cf2c7e
[20131218T17:44:21.089Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|0864b9a7-4f3f-4741-970f-8fc18e8fb815|nmutex] wait: reason=Finished op '"OpDeactivating"'. Removing from cur
[20131218T17:44:21.089Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|0864b9a7-4f3f-4741-970f-8fc18e8fb815|nmutex] About to broadcast
[20131218T17:44:21.089Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|0864b9a7-4f3f-4741-970f-8fc18e8fb815|nmutex] Done
[20131218T17:44:21.089Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|0864b9a7-4f3f-4741-970f-8fc18e8fb815|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:21.089Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|0864b9a7-4f3f-4741-970f-8fc18e8fb815|nmutex] wait: reason=Removing from current_operations
[20131218T17:44:21.089Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|0864b9a7-4f3f-4741-970f-8fc18e8fb815|vhdSlave] Removing my current op
[20131218T17:44:21.089Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|0864b9a7-4f3f-4741-970f-8fc18e8fb815|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:21.089Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|0864b9a7-4f3f-4741-970f-8fc18e8fb815|nmutex] About to broadcast
[20131218T17:44:21.089Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|0864b9a7-4f3f-4741-970f-8fc18e8fb815|nmutex] Done
[20131218T17:44:21.089Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|0864b9a7-4f3f-4741-970f-8fc18e8fb815|vhdd] Response: (omitted)
[20131218T17:44:21.090Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 411 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:21.090Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:21.090Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.detach</methodName><params><param><value><struct><member><name>dbg</name><value>dbg</value></member><member><name>dp</name><value>2ab414f7-90b6-476f-a29d-6e51d3cf2c7e</value></member><member><name>sr</name><value>1</value></member><member><name>vdi</name><value>2ab414f7-90b6-476f-a29d-6e51d3cf2c7e</value></member></struct></value></param></params></methodCall>
[20131218T17:44:21.091Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|vhdsm] API call: VDI.detach sr=1 vdi=2ab414f7-90b6-476f-a29d-6e51d3cf2c7e
[20131218T17:44:21.091Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:21.091Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|nmutex] wait: reason=Adding to current_operations
[20131218T17:44:21.091Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|vhdSlave] Checking current ops
[20131218T17:44:21.091Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:21.091Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|nmutex] wait: reason=Checking we're attached
[20131218T17:44:21.091Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|nmutex] wait: reason=Deactivating tapdisk if necessary
[20131218T17:44:21.092Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:21.092Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|nmutex] wait: reason=Executing with operation '"OpDetaching"'
[20131218T17:44:21.092Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:21.092Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|nmutex] wait: reason=Update id_map
[20131218T17:44:21.092Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|locking] update_leaf: vdi_location=2ab414f7-90b6-476f-a29d-6e51d3cf2c7e
[20131218T17:44:21.092Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|nmutex] wait: reason=Getting VHD uid='068ef7a5-2553-4065-8ace-e35267f66f1c'
[20131218T17:44:21.092Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|vhdMaster_utils] Resizing VHD uid: 068ef7a5-2553-4065-8ace-e35267f66f1c
[20131218T17:44:21.092Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-deec8f12-e51b-4297-8bda-4963a47dc8f5
[20131218T17:44:21.092Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|mlvm] Using dm_name=17ae2a00-decb-4324-bb90-9727d9c46aae (use_tmp=true)
[20131218T17:44:21.093Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|vhdutil] Dump size: overhead=4312576 phys_size=4312576 virtual_size=52428800 critical_size=56741376
[20131218T17:44:21.093Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|vhdutil] leaf_status: Some false reservation_type: Thin
[20131218T17:44:21.093Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-deec8f12-e51b-4297-8bda-4963a47dc8f5
[20131218T17:44:21.093Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|vhdMaster_utils] new_size=old_size=58720256. Not doing anything
[20131218T17:44:21.093Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|nmutex] wait: reason=Finished op '"OpDetaching"'. Removing from cur
[20131218T17:44:21.093Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|nmutex] About to broadcast
[20131218T17:44:21.093Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|nmutex] Done
[20131218T17:44:21.093Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:21.093Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|nmutex] wait: reason=Adding to master_approved_ops
[20131218T17:44:21.093Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|vhdSlave] Checking current ops
[20131218T17:44:21.093Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:21.093Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|nmutex] wait: reason=Reloading attach info
[20131218T17:44:21.093Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--deec8f12--e51b--4297--8bda--4963a47dc8f5 is now 0
[20131218T17:44:21.093Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|host] Removing LV=VG_XenStorage--1-VHD--deec8f12--e51b--4297--8bda--4963a47dc8f5
[20131218T17:44:21.094Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6 is now 0
[20131218T17:44:21.094Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|host] Removing LV=VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6
[20131218T17:44:21.094Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|nmutex] wait: reason=Removing attach info
[20131218T17:44:21.094Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:21.094Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|nmutex] wait: reason=Removing from master_approved_ops
[20131218T17:44:21.094Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|vhdSlave] Removing my current op
[20131218T17:44:21.094Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:21.094Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|nmutex] About to broadcast
[20131218T17:44:21.094Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|nmutex] Done
[20131218T17:44:21.094Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:21.094Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|nmutex] wait: reason=Removing from current_operations
[20131218T17:44:21.094Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|vhdSlave] Removing my current op
[20131218T17:44:21.094Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:21.094Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|nmutex] About to broadcast
[20131218T17:44:21.095Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|nmutex] Done
[20131218T17:44:21.095Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|0b0c4194-b677-4b18-9171-4ec3be892e33|vhdd] Response: (omitted)
[20131218T17:44:21.096Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 486 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:21.096Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:21.096Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.attach</methodName><params><param><value><struct><member><name>dbg</name><value>dbg</value></member><member><name>dp</name><value>98f55d9a-ddbc-4c06-b519-efd6b7e257fb</value></member><member><name>sr</name><value>1</value></member><member><name>vdi</name><value>98f55d9a-ddbc-4c06-b519-efd6b7e257fb</value></member><member><name>read_write</name><value><boolean>1</boolean></value></member></struct></value></param></params></methodCall>
[20131218T17:44:21.096Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|vhdsm] API call: VDI.attach sr=1 vdi=98f55d9a-ddbc-4c06-b519-efd6b7e257fb writable=true
[20131218T17:44:21.096Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:21.096Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|nmutex] wait: reason=Adding to current_operations
[20131218T17:44:21.096Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|vhdSlave] Checking current ops
[20131218T17:44:21.096Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:21.096Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|nmutex] wait: reason=Finding whether we're already attached
[20131218T17:44:21.096Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|nmutex] wait: reason=Retrieving the hashtbl of attached hosts
[20131218T17:44:21.097Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|vhdmaster] Got to the slave_attach function call
[20131218T17:44:21.097Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:21.097Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|nmutex] wait: reason=Executing with operation '"OpAttaching"'
[20131218T17:44:21.097Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:21.097Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|nmutex] wait: reason=Getting VHD uid='ee8a2145-ae74-4fc4-9790-36d8dcb4d1fb'
[20131218T17:44:21.097Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|vhdMaster_utils] Resizing VHD uid: ee8a2145-ae74-4fc4-9790-36d8dcb4d1fb
[20131218T17:44:21.097Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-8653300a-bbd4-4332-b708-b8eadc6bd5e2
[20131218T17:44:21.097Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|mlvm] Using dm_name=4a8d3187-e986-405b-bcc7-a8b823f82aa6 (use_tmp=true)
[20131218T17:44:21.097Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|vhdutil] Dump size: overhead=4312576 phys_size=4312576 virtual_size=52428800 critical_size=56741376
[20131218T17:44:21.097Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|vhdutil] leaf_status: Some true reservation_type: Attach
[20131218T17:44:21.097Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-8653300a-bbd4-4332-b708-b8eadc6bd5e2
[20131218T17:44:21.097Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|vhdMaster_utils] old_size=8388608 new_size=58720256
[20131218T17:44:21.097Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-8653300a-bbd4-4332-b708-b8eadc6bd5e2
[20131218T17:44:21.098Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|mlvm] LVM REDO: 000000000098„•¦¾   N   
   '     H¢	(VHD-8653300a-bbd4-4332-b708-b8eadc6bd5e2  #pv0 _j        !_j        @
[20131218T17:44:21.098Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-8653300a-bbd4-4332-b708-b8eadc6bd5e2
[20131218T17:44:21.098Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-8653300a-bbd4-4332-b708-b8eadc6bd5e2
[20131218T17:44:21.098Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|host] attach_lv: Got the mutex
[20131218T17:44:21.098Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|host] LV VG_XenStorage--1-VHD--8653300a--bbd4--4332--b708--b8eadc6bd5e2 not attached: attaching. refcount now 1
[20131218T17:44:21.098Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|mlvm] Using dm_name=VG_XenStorage--1-VHD--8653300a--bbd4--4332--b708--b8eadc6bd5e2 (use_tmp=false)
[20131218T17:44:21.099Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|host] attach_lv: released the mutex
[20131218T17:44:21.108Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--8653300a--bbd4--4332--b708--b8eadc6bd5e2 is now 0
[20131218T17:44:21.108Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|host] Removing LV=VG_XenStorage--1-VHD--8653300a--bbd4--4332--b708--b8eadc6bd5e2
[20131218T17:44:21.109Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|nmutex] wait: reason=Update id_map
[20131218T17:44:21.109Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|locking] update_leaf: vdi_location=98f55d9a-ddbc-4c06-b519-efd6b7e257fb
[20131218T17:44:21.109Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|nmutex] wait: reason=Finished op '"OpAttaching"'. Removing from cur
[20131218T17:44:21.109Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|nmutex] About to broadcast
[20131218T17:44:21.109Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|nmutex] Done
[20131218T17:44:21.109Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|nmutex] wait: reason=Getting a copy of a VHD chain
[20131218T17:44:21.109Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-8653300a-bbd4-4332-b708-b8eadc6bd5e2
[20131218T17:44:21.109Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-8653300a-bbd4-4332-b708-b8eadc6bd5e2
[20131218T17:44:21.109Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-8653300a-bbd4-4332-b708-b8eadc6bd5e2
[20131218T17:44:21.109Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-405c2a54-dd17-48a1-b3f5-b1eddb03e0b6
[20131218T17:44:21.109Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|vhdSlave] Got response: leaf=/tmp/dummytest/1//dev/mapper/VG_XenStorage--1-VHD--8653300a--bbd4--4332--b708--b8eadc6bd5e2
[20131218T17:44:21.109Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:21.109Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|nmutex] wait: reason=Adding to master_approved_ops
[20131218T17:44:21.109Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|vhdSlave] Checking current ops
[20131218T17:44:21.110Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:21.110Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|vhdSlave] LV name: VG_XenStorage--1-VHD--8653300a--bbd4--4332--b708--b8eadc6bd5e2
[20131218T17:44:21.110Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|vhdSlave] LV name: VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6
[20131218T17:44:21.110Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|host] attach_lv: Got the mutex
[20131218T17:44:21.110Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|host] LV VG_XenStorage--1-VHD--8653300a--bbd4--4332--b708--b8eadc6bd5e2 not attached: attaching. refcount now 1
[20131218T17:44:21.110Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|mlvm] Using dm_name=VG_XenStorage--1-VHD--8653300a--bbd4--4332--b708--b8eadc6bd5e2 (use_tmp=false)
[20131218T17:44:21.110Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|host] attach_lv: released the mutex
[20131218T17:44:21.110Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|host] attach_lv: Got the mutex
[20131218T17:44:21.110Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|host] LV VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6 not attached: attaching. refcount now 1
[20131218T17:44:21.110Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|mlvm] Using dm_name=VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6 (use_tmp=false)
[20131218T17:44:21.110Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|host] attach_lv: released the mutex
[20131218T17:44:21.111Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|vhdSlave] s_mutex lock: attach_from_sai
[20131218T17:44:21.111Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|nmutex] wait: reason=Adding info to s_attached_vdis
[20131218T17:44:21.112Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:21.112Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|nmutex] wait: reason=Removing from master_approved_ops
[20131218T17:44:21.112Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|vhdSlave] Removing my current op
[20131218T17:44:21.112Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:21.112Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|nmutex] About to broadcast
[20131218T17:44:21.112Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|nmutex] Done
[20131218T17:44:21.112Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:21.112Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|nmutex] wait: reason=Removing from current_operations
[20131218T17:44:21.112Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|vhdSlave] Removing my current op
[20131218T17:44:21.112Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:21.112Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|nmutex] About to broadcast
[20131218T17:44:21.112Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|nmutex] Done
[20131218T17:44:21.112Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|d285cad7-1da4-47c2-9c5a-34aa0dbd99ef|vhdd] Response: (omitted)
[20131218T17:44:21.114Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 413 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:21.114Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:21.114Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.activate</methodName><params><param><value><struct><member><name>dbg</name><value>dbg</value></member><member><name>dp</name><value>98f55d9a-ddbc-4c06-b519-efd6b7e257fb</value></member><member><name>sr</name><value>1</value></member><member><name>vdi</name><value>98f55d9a-ddbc-4c06-b519-efd6b7e257fb</value></member></struct></value></param></params></methodCall>
[20131218T17:44:21.114Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|vhdsm] API call: VDI.activate sr=1 vdi=98f55d9a-ddbc-4c06-b519-efd6b7e257fb
[20131218T17:44:21.114Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:21.114Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|nmutex] wait: reason=Adding to current_operations
[20131218T17:44:21.114Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|vhdSlave] Checking current ops
[20131218T17:44:21.114Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:21.114Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|nmutex] wait: reason=Checking whether the VDI is activated
[20131218T17:44:21.114Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:21.114Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|nmutex] wait: reason=Executing with operation '"OpActivating"'
[20131218T17:44:21.114Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:21.114Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|nmutex] wait: reason=Update id_map
[20131218T17:44:21.115Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|locking] update_leaf: vdi_location=98f55d9a-ddbc-4c06-b519-efd6b7e257fb
[20131218T17:44:21.115Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|nmutex] wait: reason=Finished op '"OpActivating"'. Removing from cur
[20131218T17:44:21.115Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|nmutex] About to broadcast
[20131218T17:44:21.115Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|nmutex] Done
[20131218T17:44:21.115Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:21.115Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|nmutex] wait: reason=Adding to master_approved_ops
[20131218T17:44:21.115Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|vhdSlave] Checking current ops
[20131218T17:44:21.115Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:21.115Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|nmutex] wait: reason=Activating tapdisk
[20131218T17:44:21.116Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|tapdisk_listen] Registered to listen to /dev/shm/1_1_98f55d9a-ddbc-4c06-b519-efd6b7e257fb.stats
[20131218T17:44:21.116Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|vhdSlave] Setting maxsize in shared page
[20131218T17:44:21.116Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|vhdSlave] Setting maxsize=58720256
[20131218T17:44:21.116Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:21.116Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|nmutex] wait: reason=Removing from master_approved_ops
[20131218T17:44:21.116Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|vhdSlave] Removing my current op
[20131218T17:44:21.116Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:21.116Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|nmutex] About to broadcast
[20131218T17:44:21.116Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|nmutex] Done
[20131218T17:44:21.116Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:21.116Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|nmutex] wait: reason=Removing from current_operations
[20131218T17:44:21.116Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|vhdSlave] Removing my current op
[20131218T17:44:21.116Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:21.117Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|nmutex] About to broadcast
[20131218T17:44:21.117Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|nmutex] Done
[20131218T17:44:21.117Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|5877b44a-64ab-43c7-8438-2964847e6292|vhdd] Response: (omitted)
[20131218T17:44:21.119Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|30 inet-rpc||http] Request { frame = false; method = POST; uri = /internal; query = [  ]; content_length = [ 143 ]; transfer encoding = ; version = 1.0; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = ; user_agent = xen-api-libs/1.0 }
[20131218T17:44:21.119Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|30 inet-rpc||vhdd] Internal handler
[20131218T17:44:21.119Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|30 inet-rpc||vhdd] Call={"method": "Debug.check_junk", "params": [{"sr": "1", "vdi": "98f55d9a-ddbc-4c06-b519-efd6b7e257fb", "current": [[[[0, 10]], 55]]}], "id": 932}
[20131218T17:44:21.120Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|30 inet-rpc||vhdsm] Checking junk
[20131218T17:44:21.120Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|30 inet-rpc||vhdsm] Junk OK (1)
[20131218T17:44:21.120Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|30 inet-rpc||vhdd] Response: {"result": null, "error": null, "id": 0}
[20131218T17:44:21.122Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|31 inet-rpc||http] Request { frame = false; method = POST; uri = /internal; query = [  ]; content_length = [ 128 ]; transfer encoding = ; version = 1.0; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = ; user_agent = xen-api-libs/1.0 }
[20131218T17:44:21.122Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|31 inet-rpc||vhdd] Internal handler
[20131218T17:44:21.122Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|31 inet-rpc||vhdd] Call={"method": "Debug.check_junk", "params": [{"sr": "1", "vdi": "98f55d9a-ddbc-4c06-b519-efd6b7e257fb", "current": []}], "id": 933}
[20131218T17:44:21.122Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|31 inet-rpc||vhdsm] Checking junk
[20131218T17:44:21.122Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|31 inet-rpc||vhdsm] Junk OK (0)
[20131218T17:44:21.122Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|31 inet-rpc||vhdd] Response: {"result": null, "error": null, "id": 0}
[20131218T17:44:21.124Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 415 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:21.124Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:21.124Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.deactivate</methodName><params><param><value><struct><member><name>dbg</name><value>dbg</value></member><member><name>dp</name><value>98f55d9a-ddbc-4c06-b519-efd6b7e257fb</value></member><member><name>sr</name><value>1</value></member><member><name>vdi</name><value>98f55d9a-ddbc-4c06-b519-efd6b7e257fb</value></member></struct></value></param></params></methodCall>
[20131218T17:44:21.124Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|a70b9f3e-92d6-4dc3-8384-24977bf1e15d|vhdsm] API call: VDI.deactivate sr=1 vdi=98f55d9a-ddbc-4c06-b519-efd6b7e257fb
[20131218T17:44:21.124Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|a70b9f3e-92d6-4dc3-8384-24977bf1e15d|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:21.124Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|a70b9f3e-92d6-4dc3-8384-24977bf1e15d|nmutex] wait: reason=Adding to current_operations
[20131218T17:44:21.124Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|a70b9f3e-92d6-4dc3-8384-24977bf1e15d|vhdSlave] Checking current ops
[20131218T17:44:21.124Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|a70b9f3e-92d6-4dc3-8384-24977bf1e15d|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:21.124Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|a70b9f3e-92d6-4dc3-8384-24977bf1e15d|nmutex] wait: reason=Deactivating
[20131218T17:44:21.124Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|a70b9f3e-92d6-4dc3-8384-24977bf1e15d|tapdisk_listen] Unregistering 1/98f55d9a-ddbc-4c06-b519-efd6b7e257fb
[20131218T17:44:21.125Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|a70b9f3e-92d6-4dc3-8384-24977bf1e15d|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:21.125Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|a70b9f3e-92d6-4dc3-8384-24977bf1e15d|nmutex] wait: reason=Executing with operation '"OpDeactivating"'
[20131218T17:44:21.125Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|a70b9f3e-92d6-4dc3-8384-24977bf1e15d|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:21.125Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|a70b9f3e-92d6-4dc3-8384-24977bf1e15d|nmutex] wait: reason=Update id_map
[20131218T17:44:21.125Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|a70b9f3e-92d6-4dc3-8384-24977bf1e15d|locking] update_leaf: vdi_location=98f55d9a-ddbc-4c06-b519-efd6b7e257fb
[20131218T17:44:21.125Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|a70b9f3e-92d6-4dc3-8384-24977bf1e15d|nmutex] wait: reason=Finished op '"OpDeactivating"'. Removing from cur
[20131218T17:44:21.125Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|a70b9f3e-92d6-4dc3-8384-24977bf1e15d|nmutex] About to broadcast
[20131218T17:44:21.126Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|a70b9f3e-92d6-4dc3-8384-24977bf1e15d|nmutex] Done
[20131218T17:44:21.126Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|a70b9f3e-92d6-4dc3-8384-24977bf1e15d|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:21.126Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|a70b9f3e-92d6-4dc3-8384-24977bf1e15d|nmutex] wait: reason=Removing from current_operations
[20131218T17:44:21.126Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|a70b9f3e-92d6-4dc3-8384-24977bf1e15d|vhdSlave] Removing my current op
[20131218T17:44:21.126Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|a70b9f3e-92d6-4dc3-8384-24977bf1e15d|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:21.126Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|a70b9f3e-92d6-4dc3-8384-24977bf1e15d|nmutex] About to broadcast
[20131218T17:44:21.126Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|a70b9f3e-92d6-4dc3-8384-24977bf1e15d|nmutex] Done
[20131218T17:44:21.126Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|a70b9f3e-92d6-4dc3-8384-24977bf1e15d|vhdd] Response: (omitted)
[20131218T17:44:21.128Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 411 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:21.128Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:21.128Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.detach</methodName><params><param><value><struct><member><name>dbg</name><value>dbg</value></member><member><name>dp</name><value>98f55d9a-ddbc-4c06-b519-efd6b7e257fb</value></member><member><name>sr</name><value>1</value></member><member><name>vdi</name><value>98f55d9a-ddbc-4c06-b519-efd6b7e257fb</value></member></struct></value></param></params></methodCall>
[20131218T17:44:21.128Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|vhdsm] API call: VDI.detach sr=1 vdi=98f55d9a-ddbc-4c06-b519-efd6b7e257fb
[20131218T17:44:21.128Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:21.128Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|nmutex] wait: reason=Adding to current_operations
[20131218T17:44:21.128Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|vhdSlave] Checking current ops
[20131218T17:44:21.128Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:21.128Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|nmutex] wait: reason=Checking we're attached
[20131218T17:44:21.128Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|nmutex] wait: reason=Deactivating tapdisk if necessary
[20131218T17:44:21.129Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:21.129Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|nmutex] wait: reason=Executing with operation '"OpDetaching"'
[20131218T17:44:21.129Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:21.129Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|nmutex] wait: reason=Update id_map
[20131218T17:44:21.129Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|locking] update_leaf: vdi_location=98f55d9a-ddbc-4c06-b519-efd6b7e257fb
[20131218T17:44:21.129Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|nmutex] wait: reason=Getting VHD uid='ee8a2145-ae74-4fc4-9790-36d8dcb4d1fb'
[20131218T17:44:21.129Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|vhdMaster_utils] Resizing VHD uid: ee8a2145-ae74-4fc4-9790-36d8dcb4d1fb
[20131218T17:44:21.129Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-8653300a-bbd4-4332-b708-b8eadc6bd5e2
[20131218T17:44:21.129Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|mlvm] Using dm_name=6c21eb0a-48e1-4531-8621-c9569ee278bc (use_tmp=true)
[20131218T17:44:21.152Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|vhdutil] Dump size: overhead=4312576 phys_size=4312576 virtual_size=52428800 critical_size=56741376
[20131218T17:44:21.152Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|vhdutil] leaf_status: Some false reservation_type: Attach
[20131218T17:44:21.152Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-8653300a-bbd4-4332-b708-b8eadc6bd5e2
[20131218T17:44:21.152Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|vhdMaster_utils] old_size=58720256 new_size=8388608
[20131218T17:44:21.152Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-8653300a-bbd4-4332-b708-b8eadc6bd5e2
[20131218T17:44:21.152Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|mlvm] Beginning reduce_size_to:
[20131218T17:44:21.152Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|mlvm] Lv.reduce_size_to: s.s_start_extent=0 s.s_extent_count=2 left=2
[20131218T17:44:21.153Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|mlvm] LVM REDO: 000000000078„•¦¾   :          I¡	(VHD-8653300a-bbd4-4332-b708-b8eadc6bd5e2_j        
[20131218T17:44:21.153Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-8653300a-bbd4-4332-b708-b8eadc6bd5e2
[20131218T17:44:21.153Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-8653300a-bbd4-4332-b708-b8eadc6bd5e2
[20131218T17:44:21.153Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|host] attach_lv: Got the mutex
[20131218T17:44:21.153Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|host] LV info has changed - altering dm tables
[20131218T17:44:21.153Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|host] oldty: Mlvm (VG_XenStorage--1-VHD--8653300a--bbd4--4332--b708--b8eadc6bd5e2,{"m": [{"start": 0, "len": 16384, "map": ["Linear", {"device": ["Dereferenced", "XHZVvY-fGo4-fBdW-BD3c-xAOh-F9o3-0j5s1z"], "offset": 274560}]}, {"start": 16384, "len": 98304, "map": ["Linear", {"device": ["Dereferenced", "XHZVvY-fGo4-fBdW-BD3c-xAOh-F9o3-0j5s1z"], "offset": 290944}]}]})
[20131218T17:44:21.153Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|host] newty: Mlvm (VG_XenStorage--1-VHD--8653300a--bbd4--4332--b708--b8eadc6bd5e2,{"m": [{"start": 0, "len": 16384, "map": ["Linear", {"device": ["Dereferenced", "XHZVvY-fGo4-fBdW-BD3c-xAOh-F9o3-0j5s1z"], "offset": 274560}]}]})
[20131218T17:44:21.153Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|host] attach_lv: increasing refcount for dm=VG_XenStorage--1-VHD--8653300a--bbd4--4332--b708--b8eadc6bd5e2 to 2
[20131218T17:44:21.153Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|host] attach_lv: released the mutex
[20131218T17:44:21.162Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--8653300a--bbd4--4332--b708--b8eadc6bd5e2 is now 1
[20131218T17:44:21.162Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|nmutex] wait: reason=Finished op '"OpDetaching"'. Removing from cur
[20131218T17:44:21.162Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|nmutex] About to broadcast
[20131218T17:44:21.163Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|nmutex] Done
[20131218T17:44:21.163Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:21.163Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|nmutex] wait: reason=Adding to master_approved_ops
[20131218T17:44:21.163Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|vhdSlave] Checking current ops
[20131218T17:44:21.163Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:21.163Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|nmutex] wait: reason=Reloading attach info
[20131218T17:44:21.163Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--8653300a--bbd4--4332--b708--b8eadc6bd5e2 is now 0
[20131218T17:44:21.163Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|host] Removing LV=VG_XenStorage--1-VHD--8653300a--bbd4--4332--b708--b8eadc6bd5e2
[20131218T17:44:21.163Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6 is now 0
[20131218T17:44:21.163Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|host] Removing LV=VG_XenStorage--1-VHD--405c2a54--dd17--48a1--b3f5--b1eddb03e0b6
[20131218T17:44:21.164Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|nmutex] wait: reason=Removing attach info
[20131218T17:44:21.164Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:21.164Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|nmutex] wait: reason=Removing from master_approved_ops
[20131218T17:44:21.164Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|vhdSlave] Removing my current op
[20131218T17:44:21.164Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:21.164Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|nmutex] About to broadcast
[20131218T17:44:21.164Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|nmutex] Done
[20131218T17:44:21.164Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:21.164Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|nmutex] wait: reason=Removing from current_operations
[20131218T17:44:21.164Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|vhdSlave] Removing my current op
[20131218T17:44:21.164Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:21.164Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|nmutex] About to broadcast
[20131218T17:44:21.165Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|nmutex] Done
[20131218T17:44:21.165Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|5b8b8a3f-9821-41fe-b800-3955724fcfd7|vhdd] Response: (omitted)
[20131218T17:44:21.167Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 336 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:21.167Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:21.167Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.destroy</methodName><params><param><value><struct><member><name>dbg</name><value>delete_vdi</value></member><member><name>sr</name><value>1</value></member><member><name>vdi</name><value>98f55d9a-ddbc-4c06-b519-efd6b7e257fb</value></member></struct></value></param></params></methodCall>
[20131218T17:44:21.167Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|vhdsm] API call: VDI.delete sr=1 vdi=98f55d9a-ddbc-4c06-b519-efd6b7e257fb
[20131218T17:44:21.167Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|nmutex] wait: reason=Retrieving the hashtbl of attached hosts
[20131218T17:44:21.167Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:21.167Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|nmutex] wait: reason=Executing with operation '"OpDelete"'
[20131218T17:44:21.167Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:21.167Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|nmutex] wait: reason=Removing an ID from the mapping (98f55d9a-ddbc-4c06-b519-efd6b7e257fb)
[20131218T17:44:21.167Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|lvmabs] operating on vg: VG_XenStorage-1 lv: id_to_leaf_mapping
[20131218T17:44:21.168Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|host] attach_lv: Got the mutex
[20131218T17:44:21.168Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|host] LV VG_XenStorage--1-id_to_leaf_mapping not attached: attaching. refcount now 1
[20131218T17:44:21.168Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|mlvm] Using dm_name=VG_XenStorage--1-id_to_leaf_mapping (use_tmp=false)
[20131218T17:44:21.168Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|host] attach_lv: released the mutex
[20131218T17:44:21.168Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|host] Remove LV: refcount for dm=VG_XenStorage--1-id_to_leaf_mapping is now 0
[20131218T17:44:21.168Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|host] Removing LV=VG_XenStorage--1-id_to_leaf_mapping
[20131218T17:44:21.168Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|nmutex] wait: reason=Getting VHD uid='ee8a2145-ae74-4fc4-9790-36d8dcb4d1fb'
[20131218T17:44:21.168Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|vhdMaster_utils] Calling set_hidden on VHD uid=ee8a2145-ae74-4fc4-9790-36d8dcb4d1fb
[20131218T17:44:21.168Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-8653300a-bbd4-4332-b708-b8eadc6bd5e2
[20131218T17:44:21.168Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|host] attach_lv: Got the mutex
[20131218T17:44:21.168Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|host] LV VG_XenStorage--1-VHD--8653300a--bbd4--4332--b708--b8eadc6bd5e2 not attached: attaching. refcount now 1
[20131218T17:44:21.168Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|mlvm] Using dm_name=VG_XenStorage--1-VHD--8653300a--bbd4--4332--b708--b8eadc6bd5e2 (use_tmp=false)
[20131218T17:44:21.168Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|host] attach_lv: released the mutex
[20131218T17:44:21.169Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|vhdutil] query_size_vhd: Querying size of VHD ee8a2145-ae74-4fc4-9790-36d8dcb4d1fb
[20131218T17:44:21.169Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|vhdutil] query_size_vhd: get_phys_size returned 4312576
[20131218T17:44:21.169Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|vhdutil] query_size_vhd: first_allocated_block=None
[20131218T17:44:21.169Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|vhdutil] query_size_vhd: virtual_size=52428800
[20131218T17:44:21.176Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--8653300a--bbd4--4332--b708--b8eadc6bd5e2 is now 0
[20131218T17:44:21.176Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|host] Removing LV=VG_XenStorage--1-VHD--8653300a--bbd4--4332--b708--b8eadc6bd5e2
[20131218T17:44:21.176Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|nmutex] wait: reason=Update_hidden ptr='["PVhd", "ee8a2145-ae74-4fc4-9790-36d8dcb4d1fb"]' hidden=2
[20131218T17:44:21.177Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|nmutex] wait: reason=Removing VHD uid='ee8a2145-ae74-4fc4-9790-36d8dcb4d1fb'
[20131218T17:44:21.177Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-8653300a-bbd4-4332-b708-b8eadc6bd5e2
[20131218T17:44:21.177Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|mlvm] LVM REDO: 000000000065„•¦¾   -          J”	(VHD-8653300a-bbd4-4332-b708-b8eadc6bd5e2
[20131218T17:44:21.177Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|nmutex] wait: reason=Finished op '"OpDelete"'. Removing from cur
[20131218T17:44:21.177Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|nmutex] About to broadcast
[20131218T17:44:21.177Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|nmutex] Done
[20131218T17:44:21.177Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|791f011f-4f7c-445d-9249-26ed768b525a|vhdd] Response: (omitted)
[20131218T17:44:21.179Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 336 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:21.179Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:21.179Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.destroy</methodName><params><param><value><struct><member><name>dbg</name><value>delete_vdi</value></member><member><name>sr</name><value>1</value></member><member><name>vdi</name><value>2ab414f7-90b6-476f-a29d-6e51d3cf2c7e</value></member></struct></value></param></params></methodCall>
[20131218T17:44:21.179Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|vhdsm] API call: VDI.delete sr=1 vdi=2ab414f7-90b6-476f-a29d-6e51d3cf2c7e
[20131218T17:44:21.179Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|nmutex] wait: reason=Retrieving the hashtbl of attached hosts
[20131218T17:44:21.179Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:21.179Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|nmutex] wait: reason=Executing with operation '"OpDelete"'
[20131218T17:44:21.179Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:21.179Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|nmutex] wait: reason=Removing an ID from the mapping (2ab414f7-90b6-476f-a29d-6e51d3cf2c7e)
[20131218T17:44:21.179Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|lvmabs] operating on vg: VG_XenStorage-1 lv: id_to_leaf_mapping
[20131218T17:44:21.179Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|host] attach_lv: Got the mutex
[20131218T17:44:21.179Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|host] LV VG_XenStorage--1-id_to_leaf_mapping not attached: attaching. refcount now 1
[20131218T17:44:21.179Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|mlvm] Using dm_name=VG_XenStorage--1-id_to_leaf_mapping (use_tmp=false)
[20131218T17:44:21.180Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|host] attach_lv: released the mutex
[20131218T17:44:21.180Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|host] Remove LV: refcount for dm=VG_XenStorage--1-id_to_leaf_mapping is now 0
[20131218T17:44:21.180Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|host] Removing LV=VG_XenStorage--1-id_to_leaf_mapping
[20131218T17:44:21.180Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|nmutex] wait: reason=Getting VHD uid='068ef7a5-2553-4065-8ace-e35267f66f1c'
[20131218T17:44:21.180Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|vhdMaster_utils] Calling set_hidden on VHD uid=068ef7a5-2553-4065-8ace-e35267f66f1c
[20131218T17:44:21.180Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-deec8f12-e51b-4297-8bda-4963a47dc8f5
[20131218T17:44:21.180Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|host] attach_lv: Got the mutex
[20131218T17:44:21.180Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|host] LV VG_XenStorage--1-VHD--deec8f12--e51b--4297--8bda--4963a47dc8f5 not attached: attaching. refcount now 1
[20131218T17:44:21.180Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|mlvm] Using dm_name=VG_XenStorage--1-VHD--deec8f12--e51b--4297--8bda--4963a47dc8f5 (use_tmp=false)
[20131218T17:44:21.180Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|host] attach_lv: released the mutex
[20131218T17:44:21.181Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|vhdutil] query_size_vhd: Querying size of VHD 068ef7a5-2553-4065-8ace-e35267f66f1c
[20131218T17:44:21.181Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|vhdutil] query_size_vhd: get_phys_size returned 4312576
[20131218T17:44:21.181Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|vhdutil] query_size_vhd: first_allocated_block=None
[20131218T17:44:21.181Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|vhdutil] query_size_vhd: virtual_size=52428800
[20131218T17:44:21.184Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--deec8f12--e51b--4297--8bda--4963a47dc8f5 is now 0
[20131218T17:44:21.184Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|host] Removing LV=VG_XenStorage--1-VHD--deec8f12--e51b--4297--8bda--4963a47dc8f5
[20131218T17:44:21.185Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|nmutex] wait: reason=Update_hidden ptr='["PVhd", "068ef7a5-2553-4065-8ace-e35267f66f1c"]' hidden=2
[20131218T17:44:21.185Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|nmutex] wait: reason=Removing VHD uid='068ef7a5-2553-4065-8ace-e35267f66f1c'
[20131218T17:44:21.185Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-deec8f12-e51b-4297-8bda-4963a47dc8f5
[20131218T17:44:21.185Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|mlvm] LVM REDO: 000000000065„•¦¾   -          K”	(VHD-deec8f12-e51b-4297-8bda-4963a47dc8f5
[20131218T17:44:21.185Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|nmutex] wait: reason=Finished op '"OpDelete"'. Removing from cur
[20131218T17:44:21.185Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|nmutex] About to broadcast
[20131218T17:44:21.185Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|nmutex] Done
[20131218T17:44:21.185Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|935b6211-4b5a-4dc0-a24b-ccc8d3b3fe5d|vhdd] Response: (omitted)
[20131218T17:44:21.188Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 250 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:21.188Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:21.188Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>SR.detach</methodName><params><param><value><struct><member><name>dbg</name><value>detach_all</value></member><member><name>sr</name><value>1</value></member></struct></value></param></params></methodCall>
[20131218T17:44:21.189Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|fdbe2245-e306-4bf3-9b72-04e9a7869c72|lvmabs] operating on vg: VG_XenStorage-1 lv: host_attachments
[20131218T17:44:21.189Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|fdbe2245-e306-4bf3-9b72-04e9a7869c72|host] attach_lv: Got the mutex
[20131218T17:44:21.189Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|fdbe2245-e306-4bf3-9b72-04e9a7869c72|host] LV VG_XenStorage--1-host_attachments not attached: attaching. refcount now 1
[20131218T17:44:21.189Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|fdbe2245-e306-4bf3-9b72-04e9a7869c72|mlvm] Using dm_name=VG_XenStorage--1-host_attachments (use_tmp=false)
[20131218T17:44:21.189Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|fdbe2245-e306-4bf3-9b72-04e9a7869c72|host] attach_lv: released the mutex
[20131218T17:44:21.189Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|fdbe2245-e306-4bf3-9b72-04e9a7869c72|host] Remove LV: refcount for dm=VG_XenStorage--1-host_attachments is now 0
[20131218T17:44:21.189Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|fdbe2245-e306-4bf3-9b72-04e9a7869c72|host] Removing LV=VG_XenStorage--1-host_attachments
[20131218T17:44:21.190Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|fdbe2245-e306-4bf3-9b72-04e9a7869c72|mlvm] write_label_and_pv_header:
PV header:
pvh_id: XHZVvY-fGo4-fBdW-BD3c-xAOh-F9o3-0j5s1z
pvh_device_size: 1099511627776
pvh_areas1: {offset=10551296,size=0}
pvh_areas2: {offset=4096,size=10547200}

[20131218T17:44:21.190Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|fdbe2245-e306-4bf3-9b72-04e9a7869c72|mlvm] Writing MDA header
[20131218T17:44:21.190Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|fdbe2245-e306-4bf3-9b72-04e9a7869c72|mlvm] Writing: checksum: 1176010975
magic:  LVM2 x[5A%r0N*>
version: 1
start: 4096
size: 10485760
raw_locns:[{offset:2048,size:1455,checksum:1093187049,filler:0}]

[20131218T17:44:21.191Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|fdbe2245-e306-4bf3-9b72-04e9a7869c72|host] remove_pv_id_info: Got mutex
[20131218T17:44:21.191Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|fdbe2245-e306-4bf3-9b72-04e9a7869c72|host] remove_pv_id_info: Released mutex
[20131218T17:44:21.191Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|fdbe2245-e306-4bf3-9b72-04e9a7869c72|vhdd] Response: (omitted)
[20131218T17:44:21.192Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|37 inet-rpc||http] Request { frame = false; method = POST; uri = /internal; query = [  ]; content_length = [ 66 ]; transfer encoding = ; version = 1.0; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = ; user_agent = xen-api-libs/1.0 }
[20131218T17:44:21.192Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|37 inet-rpc||vhdd] Internal handler
[20131218T17:44:21.192Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|37 inet-rpc||vhdd] Call={"method": "Debug.die", "params": [{"restart": false}], "id": 934}
[20131218T17:44:21.193Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|37 inet-rpc||vhdsm] Got instruction to die with restart=false
[20131218T17:44:21.195Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|0||watchdog] received exit code 0. Not restarting.
