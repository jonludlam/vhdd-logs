[20131218T17:44:22.316Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|0||watchdog] (Re)starting vhdd...
[20131218T17:44:22.317Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|0||watchdog] Child vhdd is: 26540
[20131218T17:44:22.318Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|0||http] Establishing inet domain server
[20131218T17:44:22.319Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|0||attachments] Ignoring ENOENT while reading attachments file
[20131218T17:44:22.321Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|9 inet-rpc||http] Request { frame = false; method = POST; uri = /internal; query = [  ]; content_length = [ 57 ]; transfer encoding = ; version = 1.0; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = ; user_agent = xen-api-libs/1.0 }
[20131218T17:44:22.321Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|9 inet-rpc||vhdd] Internal handler
[20131218T17:44:22.321Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|9 inet-rpc||vhdd] Call={"method": "Debug.get_pid", "params": [null], "id": 1262}
[20131218T17:44:22.321Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|9 inet-rpc||vhdd] Response: {"result": 26540, "error": null, "id": 0}
[20131218T17:44:22.323Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|12 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 204 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:22.323Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|12 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:22.323Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|12 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>SR.list</methodName><params><param><value><struct><member><name>dbg</name><value>wait_for_start</value></member></struct></value></param></params></methodCall>
[20131218T17:44:22.323Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|12 inet-rpc|2619e8f5-a6f1-4bc9-bfda-188e9bdb3033|vhdd] Response: (omitted)
[20131218T17:44:22.324Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|13 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 574 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:22.325Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|13 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:22.325Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|13 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>SR.create</methodName><params><param><value><struct><member><name>dbg</name><value>create_and_attach</value></member><member><name>sr</name><value>1</value></member><member><name>device_config</name><value><struct><member><name>SRmaster</name><value>true</value></member><member><name>device</name><value>/dev/dummy</value></member><member><name>reservation_mode</name><value>thin</value></member></struct></value></member><member><name>physical_size</name><value>0</value></member></struct></value></param></params></methodCall>
[20131218T17:44:22.325Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|13 inet-rpc|27786ed9-277d-4d52-9882-90030f186eaa|mlvm] write_label_and_pv_header:
PV header:
pvh_id: ExEgaJ-3nYb-glBg-7Pem-Er1s-O6gd-LcDhN7
pvh_device_size: 1099511627776
pvh_areas1: {offset=10551296,size=0}
pvh_areas2: {offset=4096,size=10547200}

[20131218T17:44:22.349Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|13 inet-rpc|27786ed9-277d-4d52-9882-90030f186eaa|mlvm] Writing MDA header
[20131218T17:44:22.398Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|13 inet-rpc|27786ed9-277d-4d52-9882-90030f186eaa|mlvm] Writing: checksum: 0
magic:  LVM2 x[5A%r0N*>
version: 1
start: 4096
size: 10485760
raw_locns:[{offset:512,size:0,checksum:0,filler:0}]

[20131218T17:44:22.403Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|13 inet-rpc|27786ed9-277d-4d52-9882-90030f186eaa|mlvm] PVs created
[20131218T17:44:22.403Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|13 inet-rpc|27786ed9-277d-4d52-9882-90030f186eaa|mlvm] write_label_and_pv_header:
PV header:
pvh_id: ExEgaJ-3nYb-glBg-7Pem-Er1s-O6gd-LcDhN7
pvh_device_size: 1099511627776
pvh_areas1: {offset=10551296,size=0}
pvh_areas2: {offset=4096,size=10547200}

[20131218T17:44:22.407Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|13 inet-rpc|27786ed9-277d-4d52-9882-90030f186eaa|mlvm] Writing MDA header
[20131218T17:44:22.407Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|13 inet-rpc|27786ed9-277d-4d52-9882-90030f186eaa|mlvm] Writing: checksum: 0
magic:  LVM2 x[5A%r0N*>
version: 1
start: 4096
size: 10485760
raw_locns:[{offset:512,size:511,checksum:2053306068,filler:0}]

[20131218T17:44:22.408Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|13 inet-rpc|27786ed9-277d-4d52-9882-90030f186eaa|mlvm] VG created
[20131218T17:44:22.408Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|13 inet-rpc|27786ed9-277d-4d52-9882-90030f186eaa|vhdd] Response: (omitted)
[20131218T17:44:22.414Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 515 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:22.414Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:22.414Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>SR.attach</methodName><params><param><value><struct><member><name>dbg</name><value>create_and_attach</value></member><member><name>sr</name><value>1</value></member><member><name>device_config</name><value><struct><member><name>SRmaster</name><value>true</value></member><member><name>device</name><value>/dev/dummy</value></member><member><name>reservation_mode</name><value>thin</value></member></struct></value></member></struct></value></param></params></methodCall>
[20131218T17:44:22.414Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|lvmabs] init_lvm: Loading Vg from devices list: [/dev/dummy]
[20131218T17:44:22.415Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|mlvm] Vg.load
[20131218T17:44:22.415Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|mlvm] Label found: 
Label header:
id: LABELONE
sector: 1
crc: 749308983
offset: 32
ty: LVM2 001

PV Header:
pvh_id: ExEgaJ-3nYb-glBg-7Pem-Er1s-O6gd-LcDhN7
pvh_device_size: 1099511627776
pvh_areas1: {offset=10551296,size=0}
pvh_areas2: {offset=4096,size=10547200}



[20131218T17:44:22.415Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|lvmabs] init_lvm: Vg loaded:
[20131218T17:44:22.415Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|lvmabs] VG_XenStorage-1 {
id = "jszjSh-87Pb-zSgG-6ZFN-vHNE-s2Vn-cXZ1xr"
seqno = 1
status = ["READ", "WRITE"]
extent_size = 8192
max_lv = 0
max_pv = 0

physical_volumes {

pv0 {
id = "ExEgaJ-3nYb-glBg-7Pem-Er1s-O6gd-LcDhN7"
device = "/dev/dummy"

status = ["ALLOCATABLE"]
dev_size = 2147483648
pe_start = 20608
pe_count = 262141
}
}

logical_volumes {
}
}
# Generated by MLVM version 0.1: 

contents = "Text Format Volume Group"
version = 1

description = ""

creation_host = "<need uname!>"
creation_time = 1387388662


[20131218T17:44:22.416Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|mlvm] write_label_and_pv_header:
PV header:
pvh_id: ExEgaJ-3nYb-glBg-7Pem-Er1s-O6gd-LcDhN7
pvh_device_size: 1099511627776
pvh_areas1: {offset=10551296,size=0}
pvh_areas2: {offset=4096,size=10547200}

[20131218T17:44:22.417Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|mlvm] Writing MDA header
[20131218T17:44:22.417Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|mlvm] Writing: checksum: -1893993421
magic:  LVM2 x[5A%r0N*>
version: 1
start: 4096
size: 10485760
raw_locns:[{offset:1024,size:738,checksum:-1990870197,filler:0}]

[20131218T17:44:22.418Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|lvmabs] init_lvm: Redo log initialized:
[20131218T17:44:22.418Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|lvmabs] VG_XenStorage-1 {
id = "jszjSh-87Pb-zSgG-6ZFN-vHNE-s2Vn-cXZ1xr"
seqno = 2
status = ["READ", "WRITE"]
extent_size = 8192
max_lv = 0
max_pv = 0

physical_volumes {

pv0 {
id = "ExEgaJ-3nYb-glBg-7Pem-Er1s-O6gd-LcDhN7"
device = "/dev/dummy"

status = ["ALLOCATABLE"]
dev_size = 2147483648
pe_start = 20608
pe_count = 262141
}
}

logical_volumes {

mlvm_redo_log {
id = "yo63pa-5BCm-rDS5-isTP-3E0v-nv2q-FIyTJF"
status = ["READ", "VISIBLE"]
segment_count = 1

segment1 {
start_extent = 0
extent_count = 1

type = "striped"
stripe_count = 1	#linear

stripes = [
"pv0", 0
]
}
}
}
}
# Generated by MLVM version 0.1: 

contents = "Text Format Volume Group"
version = 1

description = ""

creation_host = "<need uname!>"
creation_time = 1387388662


[20131218T17:44:22.418Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] add_pv_id_info: Got mutex
[20131218T17:44:22.418Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] add_pv_id_info: Released mutex
[20131218T17:44:22.418Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|vhdSlave] VhdSlave.SR.attach
[20131218T17:44:22.418Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|tapdisk] Tapdisk.scan
[20131218T17:44:22.418Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|tapdisk] parse_tapdev_link: ..
[20131218T17:44:22.418Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|tapdisk] parse_tapdev_link: .
[20131218T17:44:22.419Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|attachments] attach as slave: 1
[20131218T17:44:22.419Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|attachments] Ignoring ENOENT while reading attachments file
[20131218T17:44:22.419Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|vhdsm] mode=Master
[20131218T17:44:22.419Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|vhdmaster] m_rolling_upgrade=false
[20131218T17:44:22.419Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|lvmabs] init_lvm: Loading Vg from devices list: [/dev/dummy]
[20131218T17:44:22.419Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|mlvm] Vg.load
[20131218T17:44:22.419Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|mlvm] Label found: 
Label header:
id: LABELONE
sector: 1
crc: 749308983
offset: 32
ty: LVM2 001

PV Header:
pvh_id: ExEgaJ-3nYb-glBg-7Pem-Er1s-O6gd-LcDhN7
pvh_device_size: 1099511627776
pvh_areas1: {offset=10551296,size=0}
pvh_areas2: {offset=4096,size=10547200}



[20131218T17:44:22.420Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|mlvm] Allocations for lv mlvm_redo_log:
(pv0: [0,1])

[20131218T17:44:22.420Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|mlvm] Redo.read
[20131218T17:44:22.420Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|mlvm] start_ofs: 12 end_ofs: 12 size: 0
[20131218T17:44:22.420Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|mlvm] Reading from pos: 0
[20131218T17:44:22.420Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|mlvm] Redo.read finished
[20131218T17:44:22.420Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|lvmabs] init_lvm: Vg loaded:
[20131218T17:44:22.420Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|lvmabs] VG_XenStorage-1 {
id = "jszjSh-87Pb-zSgG-6ZFN-vHNE-s2Vn-cXZ1xr"
seqno = 2
status = ["READ", "WRITE"]
extent_size = 8192
max_lv = 0
max_pv = 0

physical_volumes {

pv0 {
id = "ExEgaJ-3nYb-glBg-7Pem-Er1s-O6gd-LcDhN7"
device = "/dev/dummy"

status = ["ALLOCATABLE"]
dev_size = 2147483648
pe_start = 20608
pe_count = 262141
}
}

logical_volumes {

mlvm_redo_log {
id = "yo63pa-5BCm-rDS5-isTP-3E0v-nv2q-FIyTJF"
status = ["READ", "VISIBLE"]
segment_count = 1

segment1 {
start_extent = 0
extent_count = 1

type = "striped"
stripe_count = 1	#linear

stripes = [
"pv0", 0
]
}
}
}
}
# Generated by MLVM version 0.1: 

contents = "Text Format Volume Group"
version = 1

description = ""

creation_host = "<need uname!>"
creation_time = 1387388662


[20131218T17:44:22.420Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|lvmabs] init_lvm: Redo log initialized:
[20131218T17:44:22.421Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|lvmabs] VG_XenStorage-1 {
id = "jszjSh-87Pb-zSgG-6ZFN-vHNE-s2Vn-cXZ1xr"
seqno = 2
status = ["READ", "WRITE"]
extent_size = 8192
max_lv = 0
max_pv = 0

physical_volumes {

pv0 {
id = "ExEgaJ-3nYb-glBg-7Pem-Er1s-O6gd-LcDhN7"
device = "/dev/dummy"

status = ["ALLOCATABLE"]
dev_size = 2147483648
pe_start = 20608
pe_count = 262141
}
}

logical_volumes {

mlvm_redo_log {
id = "yo63pa-5BCm-rDS5-isTP-3E0v-nv2q-FIyTJF"
status = ["READ", "VISIBLE"]
segment_count = 1

segment1 {
start_extent = 0
extent_count = 1

type = "striped"
stripe_count = 1	#linear

stripes = [
"pv0", 0
]
}
}
}
}
# Generated by MLVM version 0.1: 

contents = "Text Format Volume Group"
version = 1

description = ""

creation_host = "<need uname!>"
creation_time = 1387388662


[20131218T17:44:22.421Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|vhdmaster] container initialised
[20131218T17:44:22.421Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|scan] scan
[20131218T17:44:22.421Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|lvmabs] operating on vg: VG_XenStorage-1 lv: mlvm_redo_log
[20131218T17:44:22.421Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|lvmabs] operating on vg: VG_XenStorage-1 lv: mlvm_redo_log
[20131218T17:44:22.421Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|mlvm] Using dm_name=5141f388-fba8-4250-834b-bbf0c45999f3 (use_tmp=true)
[20131218T17:44:22.421Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|scan] Reading VHD: /tmp/dummytest/1/dev/mapper/5141f388-fba8-4250-834b-bbf0c45999f3
[20131218T17:44:22.422Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|mlvm] LVM REDO: 000000000113„•¦¾   ]      -   $ B 0host_attachments 	&RDW2jv-zxs1-B2LJ-SKjj-cUm4-zRZK-pd0S8a  #pv0 _j        _j        @
[20131218T17:44:22.422Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|lvmabs] operating on vg: VG_XenStorage-1 lv: host_attachments
[20131218T17:44:22.422Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|mlvm] Using dm_name=9a7b6ef1-65e5-4357-bf1c-d26973abc6df (use_tmp=true)
[20131218T17:44:22.422Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|lvmabs] operating on vg: VG_XenStorage-1 lv: host_attachments
[20131218T17:44:22.422Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] attach_lv: Got the mutex
[20131218T17:44:22.422Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] LV VG_XenStorage--1-host_attachments not attached: attaching. refcount now 1
[20131218T17:44:22.422Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|mlvm] Using dm_name=VG_XenStorage--1-host_attachments (use_tmp=false)
[20131218T17:44:22.423Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] attach_lv: released the mutex
[20131218T17:44:22.423Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] Remove LV: refcount for dm=VG_XenStorage--1-host_attachments is now 0
[20131218T17:44:22.423Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] Removing LV=VG_XenStorage--1-host_attachments
[20131218T17:44:22.424Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|mlvm] LVM REDO: 000000000115„•¦¾   _      -   $ C 2id_to_leaf_mapping 	&gvRZKS-XMQC-Rl9Q-QyLb-fsxo-g6Ad-via5DH  #pv0 _j        _j        @
[20131218T17:44:22.424Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|lvmabs] operating on vg: VG_XenStorage-1 lv: id_to_leaf_mapping
[20131218T17:44:22.424Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|mlvm] Using dm_name=8e33faab-4c3b-4d2b-90a6-1d7758e14525 (use_tmp=true)
[20131218T17:44:22.424Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|lvmabs] operating on vg: VG_XenStorage-1 lv: id_to_leaf_mapping
[20131218T17:44:22.424Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] attach_lv: Got the mutex
[20131218T17:44:22.425Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] LV VG_XenStorage--1-id_to_leaf_mapping not attached: attaching. refcount now 1
[20131218T17:44:22.425Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|mlvm] Using dm_name=VG_XenStorage--1-id_to_leaf_mapping (use_tmp=false)
[20131218T17:44:22.425Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] attach_lv: released the mutex
[20131218T17:44:22.425Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] Remove LV: refcount for dm=VG_XenStorage--1-id_to_leaf_mapping is now 0
[20131218T17:44:22.425Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] Removing LV=VG_XenStorage--1-id_to_leaf_mapping
[20131218T17:44:22.425Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|vhdmaster] Selected provisioning policy: Thin
[20131218T17:44:22.425Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|lvmabs] operating on vg: VG_XenStorage-1 lv: host_attachments
[20131218T17:44:22.425Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] attach_lv: Got the mutex
[20131218T17:44:22.426Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] LV VG_XenStorage--1-host_attachments not attached: attaching. refcount now 1
[20131218T17:44:22.426Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|mlvm] Using dm_name=VG_XenStorage--1-host_attachments (use_tmp=false)
[20131218T17:44:22.426Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] attach_lv: released the mutex
[20131218T17:44:22.426Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] Remove LV: refcount for dm=VG_XenStorage--1-host_attachments is now 0
[20131218T17:44:22.426Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] Removing LV=VG_XenStorage--1-host_attachments
[20131218T17:44:22.426Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|vhdmaster] Recovering any slaves
[20131218T17:44:22.426Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|nmutex] wait: reason=Retrieving the hashtbl of attached hosts
[20131218T17:44:22.426Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|vhdmaster] About to get_localhost()
[20131218T17:44:22.427Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|vhdmaster] Creating the attach_part_two thread
[20131218T17:44:22.427Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|vhdmaster] About to iterate over attached slaves
[20131218T17:44:22.427Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|15||vhdmaster] attach_part_two thread created
[20131218T17:44:22.427Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|vhdSlave] Registering with master
[20131218T17:44:22.427Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|nmutex] wait: reason=Finding all attached/activated VDIs
[20131218T17:44:22.427Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|15||nmutex] wait: reason=Checking whether resync is required
[20131218T17:44:22.427Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|vhdmaster] Attach from host: 1, ip: 127.0.0.1
[20131218T17:44:22.427Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|15||vhdmaster] Attach part two
[20131218T17:44:22.427Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|slave_sr_attachments] Slave SR attach
[20131218T17:44:22.428Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|nmutex] wait: reason=Adding host to the attached_hosts
[20131218T17:44:22.428Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|lvmabs] operating on vg: VG_XenStorage-1 lv: host_attachments
[20131218T17:44:22.427Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|15||nmutex] wait: reason=Getting all the leaf infos
[20131218T17:44:22.428Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] attach_lv: Got the mutex
[20131218T17:44:22.428Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] LV VG_XenStorage--1-host_attachments not attached: attaching. refcount now 1
[20131218T17:44:22.428Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|mlvm] Using dm_name=VG_XenStorage--1-host_attachments (use_tmp=false)
[20131218T17:44:22.428Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] attach_lv: released the mutex
[20131218T17:44:22.428Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] Remove LV: refcount for dm=VG_XenStorage--1-host_attachments is now 0
[20131218T17:44:22.429Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] Removing LV=VG_XenStorage--1-host_attachments
[20131218T17:44:22.429Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|nmutex] wait: reason=Resyncing VDI attachments/activations
[20131218T17:44:22.429Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|vhdSlave] Registration functions finished. Setting s_ready=true
[20131218T17:44:22.429Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|vhdsm] s_ready for the slave is: true
[20131218T17:44:22.429Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|lvmabs] operating on vg: VG_XenStorage-1 lv: host_attachments
[20131218T17:44:22.429Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] attach_lv: Got the mutex
[20131218T17:44:22.429Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] LV VG_XenStorage--1-host_attachments not attached: attaching. refcount now 1
[20131218T17:44:22.429Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|mlvm] Using dm_name=VG_XenStorage--1-host_attachments (use_tmp=false)
[20131218T17:44:22.430Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] attach_lv: released the mutex
[20131218T17:44:22.430Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] Remove LV: refcount for dm=VG_XenStorage--1-host_attachments is now 0
[20131218T17:44:22.430Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|host] Removing LV=VG_XenStorage--1-host_attachments
[20131218T17:44:22.431Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|14 inet-rpc|11bff381-41b9-40cb-9fd7-cfa2ac3e26e9|vhdd] Response: (omitted)
[20131218T17:44:22.431Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|15||vhdmaster] Resolving map inconsistencies
[20131218T17:44:22.431Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|15||vhdmaster] Resolved
[20131218T17:44:22.431Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|15||nmutex] wait: reason=setting coalesce_in_progress flag
[20131218T17:44:22.431Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|15||nmutex] wait: reason=Unsetting coalesce_in_progress flag
[20131218T17:44:22.431Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|15||nmutex] About to broadcast
[20131218T17:44:22.431Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|15||nmutex] Done
[20131218T17:44:22.446Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|16 inet-rpc||http] Request { frame = false; method = POST; uri = /internal; query = [  ]; content_length = [ 55 ]; transfer encoding = ; version = 1.0; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = ; user_agent = xen-api-libs/1.0 }
[20131218T17:44:22.446Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|16 inet-rpc||vhdd] Internal handler
[20131218T17:44:22.446Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|16 inet-rpc||vhdd] Call={"method": "SR.mode", "params": [{"sr": "1"}], "id": 1}
[20131218T17:44:22.446Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|16 inet-rpc||vhdd] Response: {"result": "Master", "error": null, "id": 0}
[20131218T17:44:22.450Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||http] Request { frame = false; method = POST; uri = /internal; query = [  ]; content_length = [ 137 ]; transfer encoding = ; version = 1.0; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = ; user_agent = xen-api-libs/1.0 }
[20131218T17:44:22.451Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||vhdd] Internal handler
[20131218T17:44:22.451Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||vhdd] Call={"method": "SR.slave_attach", "params": [{"sr": "1", "host": {"h_uuid": "2", "h_ip": "127.0.0.1", "h_port": 4095}, "vdis": {}}], "id": 2}
[20131218T17:44:22.451Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||vhdmaster] Attach from host: 2, ip: 127.0.0.1
[20131218T17:44:22.451Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||slave_sr_attachments] Slave SR attach
[20131218T17:44:22.451Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||nmutex] wait: reason=Adding host to the attached_hosts
[20131218T17:44:22.451Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||lvmabs] operating on vg: VG_XenStorage-1 lv: host_attachments
[20131218T17:44:22.451Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||host] attach_lv: Got the mutex
[20131218T17:44:22.451Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||host] LV VG_XenStorage--1-host_attachments not attached: attaching. refcount now 1
[20131218T17:44:22.451Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||mlvm] Using dm_name=VG_XenStorage--1-host_attachments (use_tmp=false)
[20131218T17:44:22.451Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||host] attach_lv: released the mutex
[20131218T17:44:22.452Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||host] Remove LV: refcount for dm=VG_XenStorage--1-host_attachments is now 0
[20131218T17:44:22.452Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||host] Removing LV=VG_XenStorage--1-host_attachments
[20131218T17:44:22.452Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||nmutex] wait: reason=Resyncing VDI attachments/activations
[20131218T17:44:22.452Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|17 inet-rpc||vhdd] Response: {"result": "OK", "error": null, "id": 0}
[20131218T17:44:22.455Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 1207 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:22.455Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:22.455Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.create</methodName><params><param><value><struct><member><name>dbg</name><value>create_vdi</value></member><member><name>sr</name><value>1</value></member><member><name>vdi_info</name><value><struct><member><name>vdi</name><value>vdi</value></member><member><name>content_id</name><value></value></member><member><name>name_label</name><value>ftest_vdi</value></member><member><name>name_description</name><value></value></member><member><name>ty</name><value>user</value></member><member><name>metadata_of_pool</name><value></value></member><member><name>is_a_snapshot</name><value><boolean>0</boolean></value></member><member><name>snapshot_time</name><value></value></member><member><name>snapshot_of</name><value></value></member><member><name>read_only</name><value><boolean>0</boolean></value></member><member><name>virtual_size</name><value>52428800</value></member><member><name>physical_utilisation</name><value>0</value></member><member><name>persistent</name><value><boolean>1</boolean></value></member><member><name>sm_config</name><value><struct></struct></value></member></struct></value></member></struct></value></param></params></methodCall>
[20131218T17:44:22.456Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|vhdsm] API call: VDI.create sr=1 size=52428800 sm_config=[]
[20131218T17:44:22.456Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|vhdutil] Dump size: overhead=4312576 phys_size=4312576 virtual_size=52428800 critical_size=109170176
[20131218T17:44:22.456Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|vhdutil] leaf_status: Some false reservation_type: Thin
[20131218T17:44:22.456Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-20c44869-83e6-4139-95f3-daa90c5fd32e
[20131218T17:44:22.456Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|mlvm] LVM REDO: 000000000138„•¦¾   v      3   ' D 	(VHD-20c44869-83e6-4139-95f3-daa90c5fd32e 	&Ls49QC-40Wi-pNbs-eEOX-CtHL-kJqj-4bLJZp  #pv0 _j        _j        @
[20131218T17:44:22.456Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-20c44869-83e6-4139-95f3-daa90c5fd32e
[20131218T17:44:22.456Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|host] attach_lv: Got the mutex
[20131218T17:44:22.456Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|host] LV VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e not attached: attaching. refcount now 1
[20131218T17:44:22.456Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|mlvm] Using dm_name=VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e (use_tmp=false)
[20131218T17:44:22.456Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|host] attach_lv: released the mutex
[20131218T17:44:22.515Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|vhdutil] query_size_vhd: Querying size of VHD 82db9c86-3c5f-44c4-8256-269eeb99aa5f
[20131218T17:44:22.515Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|vhdutil] query_size_vhd: get_phys_size returned 4311040
[20131218T17:44:22.515Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|vhdutil] query_size_vhd: first_allocated_block=None
[20131218T17:44:22.515Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|vhdutil] query_size_vhd: virtual_size=52428800
[20131218T17:44:22.517Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|nmutex] wait: reason=Adding VHD uid='82db9c86-3c5f-44c4-8256-269eeb99aa5f'
[20131218T17:44:22.517Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e is now 0
[20131218T17:44:22.517Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|host] Removing LV=VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e
[20131218T17:44:22.517Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|nmutex] wait: reason=Adding a new ID to the mapping (a90416d8-2762-44e9-b9f0-139b3a0c4c56->PVhd '82db9c86-3c5f-44c4-8256-269eeb99aa5f')
[20131218T17:44:22.517Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|lvmabs] operating on vg: VG_XenStorage-1 lv: id_to_leaf_mapping
[20131218T17:44:22.518Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|host] attach_lv: Got the mutex
[20131218T17:44:22.518Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|host] LV VG_XenStorage--1-id_to_leaf_mapping not attached: attaching. refcount now 1
[20131218T17:44:22.518Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|mlvm] Using dm_name=VG_XenStorage--1-id_to_leaf_mapping (use_tmp=false)
[20131218T17:44:22.518Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|host] attach_lv: released the mutex
[20131218T17:44:22.518Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|host] Remove LV: refcount for dm=VG_XenStorage--1-id_to_leaf_mapping is now 0
[20131218T17:44:22.518Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|host] Removing LV=VG_XenStorage--1-id_to_leaf_mapping
[20131218T17:44:22.518Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|18 inet-rpc|9bb2968b-f46c-4ee0-94fe-e9fdff6f4a59|vhdd] Response: (omitted)
[20131218T17:44:22.520Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 486 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:22.520Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:22.521Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.attach</methodName><params><param><value><struct><member><name>dbg</name><value>dbg</value></member><member><name>dp</name><value>a90416d8-2762-44e9-b9f0-139b3a0c4c56</value></member><member><name>sr</name><value>1</value></member><member><name>vdi</name><value>a90416d8-2762-44e9-b9f0-139b3a0c4c56</value></member><member><name>read_write</name><value><boolean>1</boolean></value></member></struct></value></param></params></methodCall>
[20131218T17:44:22.521Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|vhdsm] API call: VDI.attach sr=1 vdi=a90416d8-2762-44e9-b9f0-139b3a0c4c56 writable=true
[20131218T17:44:22.521Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:22.521Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|nmutex] wait: reason=Adding to current_operations
[20131218T17:44:22.521Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|vhdSlave] Checking current ops
[20131218T17:44:22.521Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:22.521Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|nmutex] wait: reason=Finding whether we're already attached
[20131218T17:44:22.521Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|nmutex] wait: reason=Retrieving the hashtbl of attached hosts
[20131218T17:44:22.521Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|vhdmaster] Got to the slave_attach function call
[20131218T17:44:22.521Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.522Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|nmutex] wait: reason=Executing with operation '"OpAttaching"'
[20131218T17:44:22.522Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.522Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|nmutex] wait: reason=Getting VHD uid='82db9c86-3c5f-44c4-8256-269eeb99aa5f'
[20131218T17:44:22.522Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|vhdMaster_utils] Resizing VHD uid: 82db9c86-3c5f-44c4-8256-269eeb99aa5f
[20131218T17:44:22.522Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-20c44869-83e6-4139-95f3-daa90c5fd32e
[20131218T17:44:22.522Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|mlvm] Using dm_name=fba24e93-187c-4ed3-8e10-3f497295cedf (use_tmp=true)
[20131218T17:44:22.523Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|vhdutil] Dump size: overhead=4311040 phys_size=4311040 virtual_size=52428800 critical_size=56739840
[20131218T17:44:22.523Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|vhdutil] leaf_status: Some true reservation_type: Thin
[20131218T17:44:22.523Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-20c44869-83e6-4139-95f3-daa90c5fd32e
[20131218T17:44:22.523Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|vhdMaster_utils] new_size=old_size=58720256. Not doing anything
[20131218T17:44:22.523Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|nmutex] wait: reason=Update id_map
[20131218T17:44:22.523Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|locking] update_leaf: vdi_location=a90416d8-2762-44e9-b9f0-139b3a0c4c56
[20131218T17:44:22.523Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|nmutex] wait: reason=Finished op '"OpAttaching"'. Removing from cur
[20131218T17:44:22.523Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|nmutex] About to broadcast
[20131218T17:44:22.523Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|nmutex] Done
[20131218T17:44:22.524Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|nmutex] wait: reason=Getting a copy of a VHD chain
[20131218T17:44:22.524Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-20c44869-83e6-4139-95f3-daa90c5fd32e
[20131218T17:44:22.524Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-20c44869-83e6-4139-95f3-daa90c5fd32e
[20131218T17:44:22.524Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-20c44869-83e6-4139-95f3-daa90c5fd32e
[20131218T17:44:22.524Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|vhdSlave] Got response: leaf=/tmp/dummytest/1//dev/mapper/VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e
[20131218T17:44:22.524Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:22.524Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|nmutex] wait: reason=Adding to master_approved_ops
[20131218T17:44:22.524Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|vhdSlave] Checking current ops
[20131218T17:44:22.524Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:22.524Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|vhdSlave] LV name: VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e
[20131218T17:44:22.525Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|host] attach_lv: Got the mutex
[20131218T17:44:22.525Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|host] LV VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e not attached: attaching. refcount now 1
[20131218T17:44:22.525Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|mlvm] Using dm_name=VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e (use_tmp=false)
[20131218T17:44:22.525Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|host] attach_lv: released the mutex
[20131218T17:44:22.526Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|vhdSlave] s_mutex lock: attach_from_sai
[20131218T17:44:22.526Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|nmutex] wait: reason=Adding info to s_attached_vdis
[20131218T17:44:22.526Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:22.526Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|nmutex] wait: reason=Removing from master_approved_ops
[20131218T17:44:22.526Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|vhdSlave] Removing my current op
[20131218T17:44:22.526Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:22.526Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|nmutex] About to broadcast
[20131218T17:44:22.526Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|nmutex] Done
[20131218T17:44:22.526Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:22.526Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|nmutex] wait: reason=Removing from current_operations
[20131218T17:44:22.526Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|vhdSlave] Removing my current op
[20131218T17:44:22.526Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:22.526Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|nmutex] About to broadcast
[20131218T17:44:22.526Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|nmutex] Done
[20131218T17:44:22.527Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|19 inet-rpc|b60361b9-0c4d-497a-a863-e128f9c9f954|vhdd] Response: (omitted)
[20131218T17:44:22.528Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 413 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:22.528Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:22.528Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.activate</methodName><params><param><value><struct><member><name>dbg</name><value>dbg</value></member><member><name>dp</name><value>a90416d8-2762-44e9-b9f0-139b3a0c4c56</value></member><member><name>sr</name><value>1</value></member><member><name>vdi</name><value>a90416d8-2762-44e9-b9f0-139b3a0c4c56</value></member></struct></value></param></params></methodCall>
[20131218T17:44:22.528Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|vhdsm] API call: VDI.activate sr=1 vdi=a90416d8-2762-44e9-b9f0-139b3a0c4c56
[20131218T17:44:22.528Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:22.528Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|nmutex] wait: reason=Adding to current_operations
[20131218T17:44:22.528Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|vhdSlave] Checking current ops
[20131218T17:44:22.528Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:22.528Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|nmutex] wait: reason=Checking whether the VDI is activated
[20131218T17:44:22.529Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.529Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|nmutex] wait: reason=Executing with operation '"OpActivating"'
[20131218T17:44:22.529Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.529Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|nmutex] wait: reason=Update id_map
[20131218T17:44:22.529Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|locking] update_leaf: vdi_location=a90416d8-2762-44e9-b9f0-139b3a0c4c56
[20131218T17:44:22.529Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|nmutex] wait: reason=Finished op '"OpActivating"'. Removing from cur
[20131218T17:44:22.529Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|nmutex] About to broadcast
[20131218T17:44:22.529Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|nmutex] Done
[20131218T17:44:22.529Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:22.529Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|nmutex] wait: reason=Adding to master_approved_ops
[20131218T17:44:22.529Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|vhdSlave] Checking current ops
[20131218T17:44:22.529Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:22.529Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|nmutex] wait: reason=Activating tapdisk
[20131218T17:44:22.530Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|tapdisk_listen] Registered to listen to /dev/shm/1_1_a90416d8-2762-44e9-b9f0-139b3a0c4c56.stats
[20131218T17:44:22.530Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|vhdSlave] Setting maxsize in shared page
[20131218T17:44:22.530Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|vhdSlave] Setting maxsize=58720256
[20131218T17:44:22.530Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:22.530Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|nmutex] wait: reason=Removing from master_approved_ops
[20131218T17:44:22.530Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|vhdSlave] Removing my current op
[20131218T17:44:22.530Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:22.530Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|nmutex] About to broadcast
[20131218T17:44:22.530Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|nmutex] Done
[20131218T17:44:22.530Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:22.530Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|nmutex] wait: reason=Removing from current_operations
[20131218T17:44:22.530Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|vhdSlave] Removing my current op
[20131218T17:44:22.530Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:22.530Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|nmutex] About to broadcast
[20131218T17:44:22.530Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|nmutex] Done
[20131218T17:44:22.530Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|20 inet-rpc|3019ef5c-2635-46c6-940b-57831d9e26a1|vhdd] Response: (omitted)
[20131218T17:44:22.532Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|21 inet-rpc||http] Request { frame = false; method = POST; uri = /internal; query = [  ]; content_length = [ 156 ]; transfer encoding = ; version = 1.0; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = ; user_agent = xen-api-libs/1.0 }
[20131218T17:44:22.532Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|21 inet-rpc||vhdd] Internal handler
[20131218T17:44:22.532Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|21 inet-rpc||vhdd] Call={"method": "Debug.write_junk", "params": [{"sr": "1", "vdi": "a90416d8-2762-44e9-b9f0-139b3a0c4c56", "size": 41943040, "n": 10, "current": []}], "id": 1263}
[20131218T17:44:22.532Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|21 inet-rpc||vhdsm] Writing junk
[20131218T17:44:22.532Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|21 inet-rpc||vhdd] Response: {"result": [], "error": null, "id": 0}
[20131218T17:44:22.534Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|22 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 326 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:22.534Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|22 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:22.534Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|22 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.stat</methodName><params><param><value><struct><member><name>dbg</name><value>dbg</value></member><member><name>sr</name><value>1</value></member><member><name>vdi</name><value>a90416d8-2762-44e9-b9f0-139b3a0c4c56</value></member></struct></value></param></params></methodCall>
[20131218T17:44:22.534Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|22 inet-rpc|6dc6f924-de08-4538-9e8f-335a29772ca7|vhdsm] API call: VDI.stat
[20131218T17:44:22.534Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|22 inet-rpc|6dc6f924-de08-4538-9e8f-335a29772ca7|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.534Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|22 inet-rpc|6dc6f924-de08-4538-9e8f-335a29772ca7|nmutex] wait: reason=Getting VHD uid='82db9c86-3c5f-44c4-8256-269eeb99aa5f'
[20131218T17:44:22.534Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|22 inet-rpc|6dc6f924-de08-4538-9e8f-335a29772ca7|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-20c44869-83e6-4139-95f3-daa90c5fd32e
[20131218T17:44:22.535Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|22 inet-rpc|6dc6f924-de08-4538-9e8f-335a29772ca7|vhdd] Response: (omitted)
[20131218T17:44:22.540Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 1257 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:22.540Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:22.540Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.clone</methodName><params><param><value><struct><member><name>dbg</name><value>dbg</value></member><member><name>sr</name><value>1</value></member><member><name>vdi_info</name><value><struct><member><name>vdi</name><value>a90416d8-2762-44e9-b9f0-139b3a0c4c56</value></member><member><name>content_id</name><value></value></member><member><name>name_label</name><value>ftest_vdi</value></member><member><name>name_description</name><value></value></member><member><name>ty</name><value>user</value></member><member><name>metadata_of_pool</name><value></value></member><member><name>is_a_snapshot</name><value><boolean>0</boolean></value></member><member><name>snapshot_time</name><value>19700101T00:00:00Z</value></member><member><name>snapshot_of</name><value></value></member><member><name>read_only</name><value><boolean>0</boolean></value></member><member><name>virtual_size</name><value>52428800</value></member><member><name>physical_utilisation</name><value>58720256</value></member><member><name>persistent</name><value><boolean>1</boolean></value></member><member><name>sm_config</name><value><struct></struct></value></member></struct></value></member></struct></value></param></params></methodCall>
[20131218T17:44:22.541Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdsm] API call: VDI.clone sr=1 vdi=a90416d8-2762-44e9-b9f0-139b3a0c4c56
[20131218T17:44:22.541Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdmaster] Clone: Checking all hosts present
[20131218T17:44:22.541Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Retrieving the hashtbl of attached hosts
[20131218T17:44:22.541Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdmaster] OK
[20131218T17:44:22.541Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.541Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Executing with operation '"OpClone"'
[20131218T17:44:22.541Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.541Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Getting VHD uid='82db9c86-3c5f-44c4-8256-269eeb99aa5f'
[20131218T17:44:22.541Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|clone] Leaf clone: creating new leaf vhd for original VDI
[20131218T17:44:22.543Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdutil] Dump size: overhead=4312576 phys_size=4312576 virtual_size=52428800 critical_size=109170176
[20131218T17:44:22.543Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdutil] leaf_status: Some true reservation_type: Thin
[20131218T17:44:22.543Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdMaster_utils] Create child: maybe about to create a LV, size=58720256
[20131218T17:44:22.543Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-d7afdbb6-8482-4d84-b369-b98731fcbe7b
[20131218T17:44:22.543Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|mlvm] LVM REDO: 000000000138„•¦¾   v      3   ' E 	(VHD-d7afdbb6-8482-4d84-b369-b98731fcbe7b 	&2VxKAO-zBSz-fG09-ifnN-QYRH-MhPK-WMFYFO  #pv0 _j        _j        @
[20131218T17:44:22.543Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdMaster_utils] Created LVM volume with uuid=d7afdbb6-8482-4d84-b369-b98731fcbe7b
[20131218T17:44:22.543Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-20c44869-83e6-4139-95f3-daa90c5fd32e
[20131218T17:44:22.543Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] attach_lv: Got the mutex
[20131218T17:44:22.543Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] LV VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e already attached
[20131218T17:44:22.543Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] attach_lv: increasing refcount for dm=VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e to 2
[20131218T17:44:22.543Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] attach_lv: released the mutex
[20131218T17:44:22.543Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-d7afdbb6-8482-4d84-b369-b98731fcbe7b
[20131218T17:44:22.543Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] attach_lv: Got the mutex
[20131218T17:44:22.543Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] LV VG_XenStorage--1-VHD--d7afdbb6--8482--4d84--b369--b98731fcbe7b not attached: attaching. refcount now 1
[20131218T17:44:22.543Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|mlvm] Using dm_name=VG_XenStorage--1-VHD--d7afdbb6--8482--4d84--b369--b98731fcbe7b (use_tmp=false)
[20131218T17:44:22.544Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] attach_lv: released the mutex
[20131218T17:44:22.591Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdutil] query_size_vhd: Querying size of VHD d5a0becd-8d46-4b2d-b45f-6e45c10b06bb
[20131218T17:44:22.591Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdutil] query_size_vhd: get_phys_size returned 4312576
[20131218T17:44:22.591Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdutil] query_size_vhd: first_allocated_block=None
[20131218T17:44:22.591Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdutil] query_size_vhd: virtual_size=52428800
[20131218T17:44:22.594Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--d7afdbb6--8482--4d84--b369--b98731fcbe7b is now 0
[20131218T17:44:22.594Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] Removing LV=VG_XenStorage--1-VHD--d7afdbb6--8482--4d84--b369--b98731fcbe7b
[20131218T17:44:22.595Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e is now 1
[20131218T17:44:22.595Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Adding VHD uid='d5a0becd-8d46-4b2d-b45f-6e45c10b06bb'
[20131218T17:44:22.595Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdMaster_utils] New VHD inserted into Hashtbl uid=d5a0becd-8d46-4b2d-b45f-6e45c10b06bb
[20131218T17:44:22.595Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Remapping an ID (a90416d8-2762-44e9-b9f0-139b3a0c4c56->PVhd 'd5a0becd-8d46-4b2d-b45f-6e45c10b06bb')
[20131218T17:44:22.595Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: id_to_leaf_mapping
[20131218T17:44:22.595Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] attach_lv: Got the mutex
[20131218T17:44:22.595Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] LV VG_XenStorage--1-id_to_leaf_mapping not attached: attaching. refcount now 1
[20131218T17:44:22.595Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|mlvm] Using dm_name=VG_XenStorage--1-id_to_leaf_mapping (use_tmp=false)
[20131218T17:44:22.595Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] attach_lv: released the mutex
[20131218T17:44:22.595Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] Remove LV: refcount for dm=VG_XenStorage--1-id_to_leaf_mapping is now 0
[20131218T17:44:22.595Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] Removing LV=VG_XenStorage--1-id_to_leaf_mapping
[20131218T17:44:22.595Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdMaster_utils] Calling reattach
[20131218T17:44:22.596Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.596Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Executing with operation '"OpReattaching"'
[20131218T17:44:22.596Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.596Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Getting VHD uid='d5a0becd-8d46-4b2d-b45f-6e45c10b06bb'
[20131218T17:44:22.596Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdMaster_utils] Resizing VHD uid: d5a0becd-8d46-4b2d-b45f-6e45c10b06bb
[20131218T17:44:22.596Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-d7afdbb6-8482-4d84-b369-b98731fcbe7b
[20131218T17:44:22.596Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|mlvm] Using dm_name=c098eaeb-93b0-4947-b5c4-31eaac3230e7 (use_tmp=true)
[20131218T17:44:22.596Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdutil] Dump size: overhead=4312576 phys_size=4312576 virtual_size=52428800 critical_size=56741376
[20131218T17:44:22.596Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdutil] leaf_status: Some true reservation_type: Thin
[20131218T17:44:22.596Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-d7afdbb6-8482-4d84-b369-b98731fcbe7b
[20131218T17:44:22.596Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdMaster_utils] new_size=old_size=58720256. Not doing anything
[20131218T17:44:22.597Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Getting a copy of a VHD chain
[20131218T17:44:22.597Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-d7afdbb6-8482-4d84-b369-b98731fcbe7b
[20131218T17:44:22.597Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-d7afdbb6-8482-4d84-b369-b98731fcbe7b
[20131218T17:44:22.597Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-d7afdbb6-8482-4d84-b369-b98731fcbe7b
[20131218T17:44:22.597Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-20c44869-83e6-4139-95f3-daa90c5fd32e
[20131218T17:44:22.597Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Finished op '"OpReattaching"'. Removing from cur
[20131218T17:44:22.597Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] About to broadcast
[20131218T17:44:22.597Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] Done
[20131218T17:44:22.597Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdMaster_utils] Reattaching host: 1
[20131218T17:44:22.597Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Find the host's IP
[20131218T17:44:22.597Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Retrieving the hashtbl of attached hosts
[20131218T17:44:22.597Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdSlave] Got slave_reload call on the following VDIs:
[20131218T17:44:22.597Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdSlave] id: a90416d8-2762-44e9-b9f0-139b3a0c4c56
[20131218T17:44:22.597Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:22.597Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Adding to master_approved_ops
[20131218T17:44:22.598Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdSlave] Checking current ops
[20131218T17:44:22.598Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:22.598Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Making sure we're attached
[20131218T17:44:22.598Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Performing inner part of reattach
[20131218T17:44:22.598Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] attach_lv: Got the mutex
[20131218T17:44:22.598Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] LV VG_XenStorage--1-VHD--d7afdbb6--8482--4d84--b369--b98731fcbe7b not attached: attaching. refcount now 1
[20131218T17:44:22.598Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|mlvm] Using dm_name=VG_XenStorage--1-VHD--d7afdbb6--8482--4d84--b369--b98731fcbe7b (use_tmp=false)
[20131218T17:44:22.598Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] attach_lv: released the mutex
[20131218T17:44:22.598Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdSlave] Pausing tapdisk...
[20131218T17:44:22.599Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:22.599Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Removing from master_approved_ops
[20131218T17:44:22.599Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdSlave] Removing my current op
[20131218T17:44:22.599Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:22.600Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] About to broadcast
[20131218T17:44:22.600Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] Done
[20131218T17:44:22.600Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|clone] Creating new leaf for new VDI
[20131218T17:44:22.600Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdutil] Dump size: overhead=4312576 phys_size=4312576 virtual_size=52428800 critical_size=109170176
[20131218T17:44:22.600Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdutil] leaf_status: Some false reservation_type: Thin
[20131218T17:44:22.600Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdMaster_utils] Create child: maybe about to create a LV, size=58720256
[20131218T17:44:22.600Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-c527f70a-35b1-414d-9fe1-2952e749b6ad
[20131218T17:44:22.601Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|mlvm] LVM REDO: 000000000138„•¦¾   v      3   ' F 	(VHD-c527f70a-35b1-414d-9fe1-2952e749b6ad 	&PVCDbt-lJAV-fFIe-87IK-OmfQ-3kKg-Gy1VzF  #pv0 _j        _j        @
[20131218T17:44:22.601Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdMaster_utils] Created LVM volume with uuid=c527f70a-35b1-414d-9fe1-2952e749b6ad
[20131218T17:44:22.601Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-20c44869-83e6-4139-95f3-daa90c5fd32e
[20131218T17:44:22.601Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] attach_lv: Got the mutex
[20131218T17:44:22.601Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] LV VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e already attached
[20131218T17:44:22.601Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] attach_lv: increasing refcount for dm=VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e to 2
[20131218T17:44:22.601Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] attach_lv: released the mutex
[20131218T17:44:22.601Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-c527f70a-35b1-414d-9fe1-2952e749b6ad
[20131218T17:44:22.601Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] attach_lv: Got the mutex
[20131218T17:44:22.601Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] LV VG_XenStorage--1-VHD--c527f70a--35b1--414d--9fe1--2952e749b6ad not attached: attaching. refcount now 1
[20131218T17:44:22.601Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|mlvm] Using dm_name=VG_XenStorage--1-VHD--c527f70a--35b1--414d--9fe1--2952e749b6ad (use_tmp=false)
[20131218T17:44:22.601Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] attach_lv: released the mutex
[20131218T17:44:22.647Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdutil] query_size_vhd: Querying size of VHD 0f8513df-e3d6-4642-b24c-6e68c935d1a3
[20131218T17:44:22.647Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdutil] query_size_vhd: get_phys_size returned 4312576
[20131218T17:44:22.647Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdutil] query_size_vhd: first_allocated_block=None
[20131218T17:44:22.647Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdutil] query_size_vhd: virtual_size=52428800
[20131218T17:44:22.651Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--c527f70a--35b1--414d--9fe1--2952e749b6ad is now 0
[20131218T17:44:22.652Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] Removing LV=VG_XenStorage--1-VHD--c527f70a--35b1--414d--9fe1--2952e749b6ad
[20131218T17:44:22.652Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e is now 1
[20131218T17:44:22.652Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Adding VHD uid='0f8513df-e3d6-4642-b24c-6e68c935d1a3'
[20131218T17:44:22.652Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdMaster_utils] New VHD inserted into Hashtbl uid=0f8513df-e3d6-4642-b24c-6e68c935d1a3
[20131218T17:44:22.652Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Getting VHD uid='82db9c86-3c5f-44c4-8256-269eeb99aa5f'
[20131218T17:44:22.652Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdMaster_utils] Calling set_hidden on VHD uid=82db9c86-3c5f-44c4-8256-269eeb99aa5f
[20131218T17:44:22.652Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-20c44869-83e6-4139-95f3-daa90c5fd32e
[20131218T17:44:22.652Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] attach_lv: Got the mutex
[20131218T17:44:22.652Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] LV VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e already attached
[20131218T17:44:22.652Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] attach_lv: increasing refcount for dm=VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e to 2
[20131218T17:44:22.652Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] attach_lv: released the mutex
[20131218T17:44:22.652Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdutil] query_size_vhd: Querying size of VHD 82db9c86-3c5f-44c4-8256-269eeb99aa5f
[20131218T17:44:22.652Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdutil] query_size_vhd: get_phys_size returned 4311040
[20131218T17:44:22.652Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdutil] query_size_vhd: first_allocated_block=None
[20131218T17:44:22.652Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdutil] query_size_vhd: virtual_size=52428800
[20131218T17:44:22.659Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e is now 1
[20131218T17:44:22.659Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdMaster_utils] Resizing VHD uid: 82db9c86-3c5f-44c4-8256-269eeb99aa5f
[20131218T17:44:22.659Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-20c44869-83e6-4139-95f3-daa90c5fd32e
[20131218T17:44:22.659Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|mlvm] Using dm_name=c21f07cd-522b-4565-9317-1ce46ccb9404 (use_tmp=true)
[20131218T17:44:22.663Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdutil] Dump size: overhead=4311040 phys_size=4311040 virtual_size=52428800 critical_size=56739840
[20131218T17:44:22.663Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdutil] leaf_status: None reservation_type: Thin
[20131218T17:44:22.663Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-20c44869-83e6-4139-95f3-daa90c5fd32e
[20131218T17:44:22.663Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdMaster_utils] old_size=58720256 new_size=8388608
[20131218T17:44:22.663Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-20c44869-83e6-4139-95f3-daa90c5fd32e
[20131218T17:44:22.663Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|mlvm] Beginning reduce_size_to:
[20131218T17:44:22.663Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|mlvm] Lv.reduce_size_to: s.s_start_extent=0 s.s_extent_count=14 left=2
[20131218T17:44:22.663Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|mlvm] LVM REDO: 000000000078„•¦¾   :          G¡	(VHD-20c44869-83e6-4139-95f3-daa90c5fd32e_j        
[20131218T17:44:22.663Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-20c44869-83e6-4139-95f3-daa90c5fd32e
[20131218T17:44:22.663Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-20c44869-83e6-4139-95f3-daa90c5fd32e
[20131218T17:44:22.663Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] attach_lv: Got the mutex
[20131218T17:44:22.663Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] LV info has changed - altering dm tables
[20131218T17:44:22.664Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] oldty: Mlvm (VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e,{"m": [{"start": 0, "len": 114688, "map": ["Linear", {"device": ["Dereferenced", "ExEgaJ-3nYb-glBg-7Pem-Er1s-O6gd-LcDhN7"], "offset": 45184}]}]})
[20131218T17:44:22.664Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] newty: Mlvm (VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e,{"m": [{"start": 0, "len": 16384, "map": ["Linear", {"device": ["Dereferenced", "ExEgaJ-3nYb-glBg-7Pem-Er1s-O6gd-LcDhN7"], "offset": 45184}]}]})
[20131218T17:44:22.664Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] attach_lv: increasing refcount for dm=VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e to 2
[20131218T17:44:22.664Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] attach_lv: released the mutex
[20131218T17:44:22.669Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e is now 1
[20131218T17:44:22.673Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdMaster_utils] Calling reattach
[20131218T17:44:22.673Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.673Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Executing with operation '"OpReattaching"'
[20131218T17:44:22.673Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.673Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Getting VHD uid='d5a0becd-8d46-4b2d-b45f-6e45c10b06bb'
[20131218T17:44:22.673Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdMaster_utils] Resizing VHD uid: d5a0becd-8d46-4b2d-b45f-6e45c10b06bb
[20131218T17:44:22.673Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-d7afdbb6-8482-4d84-b369-b98731fcbe7b
[20131218T17:44:22.673Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|mlvm] Using dm_name=42460a02-03b3-4527-b9ec-dff2ff7e52a2 (use_tmp=true)
[20131218T17:44:22.674Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdutil] Dump size: overhead=4312576 phys_size=4312576 virtual_size=52428800 critical_size=56741376
[20131218T17:44:22.674Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdutil] leaf_status: Some true reservation_type: Thin
[20131218T17:44:22.674Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-d7afdbb6-8482-4d84-b369-b98731fcbe7b
[20131218T17:44:22.674Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdMaster_utils] new_size=old_size=58720256. Not doing anything
[20131218T17:44:22.674Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Getting a copy of a VHD chain
[20131218T17:44:22.674Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-d7afdbb6-8482-4d84-b369-b98731fcbe7b
[20131218T17:44:22.674Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-d7afdbb6-8482-4d84-b369-b98731fcbe7b
[20131218T17:44:22.674Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-d7afdbb6-8482-4d84-b369-b98731fcbe7b
[20131218T17:44:22.674Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-20c44869-83e6-4139-95f3-daa90c5fd32e
[20131218T17:44:22.674Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Finished op '"OpReattaching"'. Removing from cur
[20131218T17:44:22.674Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] About to broadcast
[20131218T17:44:22.674Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] Done
[20131218T17:44:22.674Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdMaster_utils] Reattaching host: 1
[20131218T17:44:22.674Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Find the host's IP
[20131218T17:44:22.674Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Retrieving the hashtbl of attached hosts
[20131218T17:44:22.674Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdSlave] Got slave_reload call on the following VDIs:
[20131218T17:44:22.674Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdSlave] id: a90416d8-2762-44e9-b9f0-139b3a0c4c56
[20131218T17:44:22.674Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:22.675Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Adding to master_approved_ops
[20131218T17:44:22.675Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdSlave] Checking current ops
[20131218T17:44:22.675Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:22.675Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Making sure we're attached
[20131218T17:44:22.675Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Performing inner part of reattach
[20131218T17:44:22.675Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdSlave] Pausing tapdisk...
[20131218T17:44:22.675Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] change_lv: Got the mutex
[20131218T17:44:22.675Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] change_lv: Released the mutex
[20131218T17:44:22.676Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:22.676Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Removing from master_approved_ops
[20131218T17:44:22.676Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdSlave] Removing my current op
[20131218T17:44:22.676Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:22.676Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] About to broadcast
[20131218T17:44:22.676Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] Done
[20131218T17:44:22.676Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Update_vhd_size uid='82db9c86-3c5f-44c4-8256-269eeb99aa5f'
[20131218T17:44:22.677Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhd_records] newsize=overhead=4311040 phys_size=4311040 virtual_size=52428800 critical_size=56739840
[20131218T17:44:22.677Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Update_hidden ptr='["PVhd", "82db9c86-3c5f-44c4-8256-269eeb99aa5f"]' hidden=2
[20131218T17:44:22.677Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|clone] Marked vhd: 82db9c86-3c5f-44c4-8256-269eeb99aa5f as hidden=2
[20131218T17:44:22.677Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|coalesce] Beginning relink phase of vhd: 82db9c86-3c5f-44c4-8256-269eeb99aa5f
[20131218T17:44:22.677Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Finding the children of parent ["PVhd", "82db9c86-3c5f-44c4-8256-269eeb99aa5f"]
[20131218T17:44:22.677Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Getting VHD uid='82db9c86-3c5f-44c4-8256-269eeb99aa5f'
[20131218T17:44:22.677Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Adding a new ID to the mapping (3aa413f6-76f3-4b22-9723-e49feda81d54->PVhd '0f8513df-e3d6-4642-b24c-6e68c935d1a3')
[20131218T17:44:22.677Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|lvmabs] operating on vg: VG_XenStorage-1 lv: id_to_leaf_mapping
[20131218T17:44:22.677Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] attach_lv: Got the mutex
[20131218T17:44:22.677Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] LV VG_XenStorage--1-id_to_leaf_mapping not attached: attaching. refcount now 1
[20131218T17:44:22.677Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|mlvm] Using dm_name=VG_XenStorage--1-id_to_leaf_mapping (use_tmp=false)
[20131218T17:44:22.677Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] attach_lv: released the mutex
[20131218T17:44:22.678Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] Remove LV: refcount for dm=VG_XenStorage--1-id_to_leaf_mapping is now 0
[20131218T17:44:22.678Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|host] Removing LV=VG_XenStorage--1-id_to_leaf_mapping
[20131218T17:44:22.678Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] wait: reason=Finished op '"OpClone"'. Removing from cur
[20131218T17:44:22.678Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] About to broadcast
[20131218T17:44:22.678Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|nmutex] Done
[20131218T17:44:22.678Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|23 inet-rpc|435cf187-c22b-4d16-8a09-0de57dcc4043|vhdd] Response: (omitted)
[20131218T17:44:22.680Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|24 inet-rpc||http] Request { frame = false; method = POST; uri = /internal; query = [  ]; content_length = [ 144 ]; transfer encoding = ; version = 1.0; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = ; user_agent = xen-api-libs/1.0 }
[20131218T17:44:22.680Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|24 inet-rpc||vhdd] Internal handler
[20131218T17:44:22.680Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|24 inet-rpc||vhdd] Call={"method": "Debug.check_junk", "params": [{"sr": "1", "vdi": "a90416d8-2762-44e9-b9f0-139b3a0c4c56", "current": [[[[0, 10]], 55]]}], "id": 1264}
[20131218T17:44:22.680Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|24 inet-rpc||vhdsm] Checking junk
[20131218T17:44:22.680Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|24 inet-rpc||vhdsm] Junk OK (1)
[20131218T17:44:22.680Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|24 inet-rpc||vhdd] Response: {"result": null, "error": null, "id": 0}
[20131218T17:44:22.682Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|25 inet-rpc||http] Request { frame = false; method = POST; uri = /internal; query = [  ]; content_length = [ 129 ]; transfer encoding = ; version = 1.0; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = ; user_agent = xen-api-libs/1.0 }
[20131218T17:44:22.682Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|25 inet-rpc||vhdd] Internal handler
[20131218T17:44:22.682Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|25 inet-rpc||vhdd] Call={"method": "Debug.check_junk", "params": [{"sr": "1", "vdi": "a90416d8-2762-44e9-b9f0-139b3a0c4c56", "current": []}], "id": 1265}
[20131218T17:44:22.682Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|25 inet-rpc||vhdsm] Checking junk
[20131218T17:44:22.682Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|25 inet-rpc||vhdsm] Junk OK (0)
[20131218T17:44:22.682Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|25 inet-rpc||vhdd] Response: {"result": null, "error": null, "id": 0}
[20131218T17:44:22.683Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 415 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:22.684Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:22.684Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.deactivate</methodName><params><param><value><struct><member><name>dbg</name><value>dbg</value></member><member><name>dp</name><value>a90416d8-2762-44e9-b9f0-139b3a0c4c56</value></member><member><name>sr</name><value>1</value></member><member><name>vdi</name><value>a90416d8-2762-44e9-b9f0-139b3a0c4c56</value></member></struct></value></param></params></methodCall>
[20131218T17:44:22.684Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|da824076-c131-47ec-bbe1-a2a360d80c01|vhdsm] API call: VDI.deactivate sr=1 vdi=a90416d8-2762-44e9-b9f0-139b3a0c4c56
[20131218T17:44:22.684Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|da824076-c131-47ec-bbe1-a2a360d80c01|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:22.684Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|da824076-c131-47ec-bbe1-a2a360d80c01|nmutex] wait: reason=Adding to current_operations
[20131218T17:44:22.684Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|da824076-c131-47ec-bbe1-a2a360d80c01|vhdSlave] Checking current ops
[20131218T17:44:22.684Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|da824076-c131-47ec-bbe1-a2a360d80c01|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:22.684Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|da824076-c131-47ec-bbe1-a2a360d80c01|nmutex] wait: reason=Deactivating
[20131218T17:44:22.684Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|da824076-c131-47ec-bbe1-a2a360d80c01|tapdisk_listen] Unregistering 1/a90416d8-2762-44e9-b9f0-139b3a0c4c56
[20131218T17:44:22.685Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|da824076-c131-47ec-bbe1-a2a360d80c01|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.685Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|da824076-c131-47ec-bbe1-a2a360d80c01|nmutex] wait: reason=Executing with operation '"OpDeactivating"'
[20131218T17:44:22.685Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|da824076-c131-47ec-bbe1-a2a360d80c01|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.685Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|da824076-c131-47ec-bbe1-a2a360d80c01|nmutex] wait: reason=Update id_map
[20131218T17:44:22.685Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|da824076-c131-47ec-bbe1-a2a360d80c01|locking] update_leaf: vdi_location=a90416d8-2762-44e9-b9f0-139b3a0c4c56
[20131218T17:44:22.685Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|da824076-c131-47ec-bbe1-a2a360d80c01|nmutex] wait: reason=Finished op '"OpDeactivating"'. Removing from cur
[20131218T17:44:22.685Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|da824076-c131-47ec-bbe1-a2a360d80c01|nmutex] About to broadcast
[20131218T17:44:22.685Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|da824076-c131-47ec-bbe1-a2a360d80c01|nmutex] Done
[20131218T17:44:22.685Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|da824076-c131-47ec-bbe1-a2a360d80c01|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:22.685Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|da824076-c131-47ec-bbe1-a2a360d80c01|nmutex] wait: reason=Removing from current_operations
[20131218T17:44:22.685Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|da824076-c131-47ec-bbe1-a2a360d80c01|vhdSlave] Removing my current op
[20131218T17:44:22.685Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|da824076-c131-47ec-bbe1-a2a360d80c01|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:22.685Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|da824076-c131-47ec-bbe1-a2a360d80c01|nmutex] About to broadcast
[20131218T17:44:22.685Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|da824076-c131-47ec-bbe1-a2a360d80c01|nmutex] Done
[20131218T17:44:22.685Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|26 inet-rpc|da824076-c131-47ec-bbe1-a2a360d80c01|vhdd] Response: (omitted)
[20131218T17:44:22.686Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 411 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:22.686Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:22.686Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.detach</methodName><params><param><value><struct><member><name>dbg</name><value>dbg</value></member><member><name>dp</name><value>a90416d8-2762-44e9-b9f0-139b3a0c4c56</value></member><member><name>sr</name><value>1</value></member><member><name>vdi</name><value>a90416d8-2762-44e9-b9f0-139b3a0c4c56</value></member></struct></value></param></params></methodCall>
[20131218T17:44:22.687Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|vhdsm] API call: VDI.detach sr=1 vdi=a90416d8-2762-44e9-b9f0-139b3a0c4c56
[20131218T17:44:22.687Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:22.687Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|nmutex] wait: reason=Adding to current_operations
[20131218T17:44:22.687Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|vhdSlave] Checking current ops
[20131218T17:44:22.687Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:22.687Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|nmutex] wait: reason=Checking we're attached
[20131218T17:44:22.687Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|nmutex] wait: reason=Deactivating tapdisk if necessary
[20131218T17:44:22.688Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.688Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|nmutex] wait: reason=Executing with operation '"OpDetaching"'
[20131218T17:44:22.688Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.688Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|nmutex] wait: reason=Update id_map
[20131218T17:44:22.688Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|locking] update_leaf: vdi_location=a90416d8-2762-44e9-b9f0-139b3a0c4c56
[20131218T17:44:22.688Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|nmutex] wait: reason=Getting VHD uid='d5a0becd-8d46-4b2d-b45f-6e45c10b06bb'
[20131218T17:44:22.688Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|vhdMaster_utils] Resizing VHD uid: d5a0becd-8d46-4b2d-b45f-6e45c10b06bb
[20131218T17:44:22.688Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-d7afdbb6-8482-4d84-b369-b98731fcbe7b
[20131218T17:44:22.688Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|mlvm] Using dm_name=c14904f2-85f3-4cb1-81ad-11e7e1210203 (use_tmp=true)
[20131218T17:44:22.689Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|vhdutil] Dump size: overhead=4312576 phys_size=4312576 virtual_size=52428800 critical_size=56741376
[20131218T17:44:22.689Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|vhdutil] leaf_status: Some false reservation_type: Thin
[20131218T17:44:22.689Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-d7afdbb6-8482-4d84-b369-b98731fcbe7b
[20131218T17:44:22.689Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|vhdMaster_utils] new_size=old_size=58720256. Not doing anything
[20131218T17:44:22.689Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|nmutex] wait: reason=Finished op '"OpDetaching"'. Removing from cur
[20131218T17:44:22.689Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|nmutex] About to broadcast
[20131218T17:44:22.689Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|nmutex] Done
[20131218T17:44:22.689Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:22.689Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|nmutex] wait: reason=Adding to master_approved_ops
[20131218T17:44:22.689Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|vhdSlave] Checking current ops
[20131218T17:44:22.689Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:22.689Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|nmutex] wait: reason=Reloading attach info
[20131218T17:44:22.689Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--d7afdbb6--8482--4d84--b369--b98731fcbe7b is now 0
[20131218T17:44:22.689Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|host] Removing LV=VG_XenStorage--1-VHD--d7afdbb6--8482--4d84--b369--b98731fcbe7b
[20131218T17:44:22.689Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e is now 0
[20131218T17:44:22.689Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|host] Removing LV=VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e
[20131218T17:44:22.689Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|nmutex] wait: reason=Removing attach info
[20131218T17:44:22.690Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:22.690Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|nmutex] wait: reason=Removing from master_approved_ops
[20131218T17:44:22.690Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|vhdSlave] Removing my current op
[20131218T17:44:22.690Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:22.690Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|nmutex] About to broadcast
[20131218T17:44:22.690Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|nmutex] Done
[20131218T17:44:22.690Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:22.690Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|nmutex] wait: reason=Removing from current_operations
[20131218T17:44:22.690Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|vhdSlave] Removing my current op
[20131218T17:44:22.690Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:22.690Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|nmutex] About to broadcast
[20131218T17:44:22.690Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|nmutex] Done
[20131218T17:44:22.690Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|27 inet-rpc|2a69775c-bd8d-4652-829e-270fe2551434|vhdd] Response: (omitted)
[20131218T17:44:22.691Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 486 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:22.691Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:22.691Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.attach</methodName><params><param><value><struct><member><name>dbg</name><value>dbg</value></member><member><name>dp</name><value>3aa413f6-76f3-4b22-9723-e49feda81d54</value></member><member><name>sr</name><value>1</value></member><member><name>vdi</name><value>3aa413f6-76f3-4b22-9723-e49feda81d54</value></member><member><name>read_write</name><value><boolean>1</boolean></value></member></struct></value></param></params></methodCall>
[20131218T17:44:22.691Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|vhdsm] API call: VDI.attach sr=1 vdi=3aa413f6-76f3-4b22-9723-e49feda81d54 writable=true
[20131218T17:44:22.692Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:22.692Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|nmutex] wait: reason=Adding to current_operations
[20131218T17:44:22.692Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|vhdSlave] Checking current ops
[20131218T17:44:22.692Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:22.692Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|nmutex] wait: reason=Finding whether we're already attached
[20131218T17:44:22.692Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|nmutex] wait: reason=Retrieving the hashtbl of attached hosts
[20131218T17:44:22.692Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|vhdmaster] Got to the slave_attach function call
[20131218T17:44:22.692Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.692Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|nmutex] wait: reason=Executing with operation '"OpAttaching"'
[20131218T17:44:22.692Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.692Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|nmutex] wait: reason=Getting VHD uid='0f8513df-e3d6-4642-b24c-6e68c935d1a3'
[20131218T17:44:22.692Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|vhdMaster_utils] Resizing VHD uid: 0f8513df-e3d6-4642-b24c-6e68c935d1a3
[20131218T17:44:22.692Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-c527f70a-35b1-414d-9fe1-2952e749b6ad
[20131218T17:44:22.692Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|mlvm] Using dm_name=ee2b6c8c-4aa5-4069-825d-86db60da54b2 (use_tmp=true)
[20131218T17:44:22.692Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|vhdutil] Dump size: overhead=4312576 phys_size=4312576 virtual_size=52428800 critical_size=56741376
[20131218T17:44:22.692Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|vhdutil] leaf_status: Some true reservation_type: Thin
[20131218T17:44:22.693Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-c527f70a-35b1-414d-9fe1-2952e749b6ad
[20131218T17:44:22.693Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|vhdMaster_utils] new_size=old_size=58720256. Not doing anything
[20131218T17:44:22.693Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|nmutex] wait: reason=Update id_map
[20131218T17:44:22.693Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|locking] update_leaf: vdi_location=3aa413f6-76f3-4b22-9723-e49feda81d54
[20131218T17:44:22.693Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|nmutex] wait: reason=Finished op '"OpAttaching"'. Removing from cur
[20131218T17:44:22.693Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|nmutex] About to broadcast
[20131218T17:44:22.693Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|nmutex] Done
[20131218T17:44:22.693Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|nmutex] wait: reason=Getting a copy of a VHD chain
[20131218T17:44:22.693Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-c527f70a-35b1-414d-9fe1-2952e749b6ad
[20131218T17:44:22.693Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-c527f70a-35b1-414d-9fe1-2952e749b6ad
[20131218T17:44:22.693Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-c527f70a-35b1-414d-9fe1-2952e749b6ad
[20131218T17:44:22.693Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-20c44869-83e6-4139-95f3-daa90c5fd32e
[20131218T17:44:22.693Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|vhdSlave] Got response: leaf=/tmp/dummytest/1//dev/mapper/VG_XenStorage--1-VHD--c527f70a--35b1--414d--9fe1--2952e749b6ad
[20131218T17:44:22.693Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:22.693Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|nmutex] wait: reason=Adding to master_approved_ops
[20131218T17:44:22.693Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|vhdSlave] Checking current ops
[20131218T17:44:22.693Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:22.694Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|vhdSlave] LV name: VG_XenStorage--1-VHD--c527f70a--35b1--414d--9fe1--2952e749b6ad
[20131218T17:44:22.694Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|vhdSlave] LV name: VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e
[20131218T17:44:22.694Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|host] attach_lv: Got the mutex
[20131218T17:44:22.694Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|host] LV VG_XenStorage--1-VHD--c527f70a--35b1--414d--9fe1--2952e749b6ad not attached: attaching. refcount now 1
[20131218T17:44:22.694Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|mlvm] Using dm_name=VG_XenStorage--1-VHD--c527f70a--35b1--414d--9fe1--2952e749b6ad (use_tmp=false)
[20131218T17:44:22.694Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|host] attach_lv: released the mutex
[20131218T17:44:22.694Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|host] attach_lv: Got the mutex
[20131218T17:44:22.694Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|host] LV VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e not attached: attaching. refcount now 1
[20131218T17:44:22.694Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|mlvm] Using dm_name=VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e (use_tmp=false)
[20131218T17:44:22.694Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|host] attach_lv: released the mutex
[20131218T17:44:22.695Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|vhdSlave] s_mutex lock: attach_from_sai
[20131218T17:44:22.695Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|nmutex] wait: reason=Adding info to s_attached_vdis
[20131218T17:44:22.696Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:22.696Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|nmutex] wait: reason=Removing from master_approved_ops
[20131218T17:44:22.696Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|vhdSlave] Removing my current op
[20131218T17:44:22.696Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:22.696Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|nmutex] About to broadcast
[20131218T17:44:22.696Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|nmutex] Done
[20131218T17:44:22.696Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:22.696Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|nmutex] wait: reason=Removing from current_operations
[20131218T17:44:22.696Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|vhdSlave] Removing my current op
[20131218T17:44:22.696Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:22.696Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|nmutex] About to broadcast
[20131218T17:44:22.696Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|nmutex] Done
[20131218T17:44:22.696Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|28 inet-rpc|25f4c8b8-041d-4530-a847-bfc670c5fe79|vhdd] Response: (omitted)
[20131218T17:44:22.698Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 413 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:22.698Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:22.698Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.activate</methodName><params><param><value><struct><member><name>dbg</name><value>dbg</value></member><member><name>dp</name><value>3aa413f6-76f3-4b22-9723-e49feda81d54</value></member><member><name>sr</name><value>1</value></member><member><name>vdi</name><value>3aa413f6-76f3-4b22-9723-e49feda81d54</value></member></struct></value></param></params></methodCall>
[20131218T17:44:22.698Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|vhdsm] API call: VDI.activate sr=1 vdi=3aa413f6-76f3-4b22-9723-e49feda81d54
[20131218T17:44:22.698Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:22.698Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|nmutex] wait: reason=Adding to current_operations
[20131218T17:44:22.698Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|vhdSlave] Checking current ops
[20131218T17:44:22.698Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:22.698Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|nmutex] wait: reason=Checking whether the VDI is activated
[20131218T17:44:22.698Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.699Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|nmutex] wait: reason=Executing with operation '"OpActivating"'
[20131218T17:44:22.699Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.699Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|nmutex] wait: reason=Update id_map
[20131218T17:44:22.699Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|locking] update_leaf: vdi_location=3aa413f6-76f3-4b22-9723-e49feda81d54
[20131218T17:44:22.699Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|nmutex] wait: reason=Finished op '"OpActivating"'. Removing from cur
[20131218T17:44:22.699Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|nmutex] About to broadcast
[20131218T17:44:22.699Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|nmutex] Done
[20131218T17:44:22.699Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:22.699Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|nmutex] wait: reason=Adding to master_approved_ops
[20131218T17:44:22.699Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|vhdSlave] Checking current ops
[20131218T17:44:22.699Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:22.699Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|nmutex] wait: reason=Activating tapdisk
[20131218T17:44:22.699Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|tapdisk_listen] Registered to listen to /dev/shm/1_1_3aa413f6-76f3-4b22-9723-e49feda81d54.stats
[20131218T17:44:22.700Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|vhdSlave] Setting maxsize in shared page
[20131218T17:44:22.700Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|vhdSlave] Setting maxsize=58720256
[20131218T17:44:22.700Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:22.700Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|nmutex] wait: reason=Removing from master_approved_ops
[20131218T17:44:22.700Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|vhdSlave] Removing my current op
[20131218T17:44:22.700Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:22.700Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|nmutex] About to broadcast
[20131218T17:44:22.700Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|nmutex] Done
[20131218T17:44:22.700Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:22.700Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|nmutex] wait: reason=Removing from current_operations
[20131218T17:44:22.700Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|vhdSlave] Removing my current op
[20131218T17:44:22.700Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:22.700Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|nmutex] About to broadcast
[20131218T17:44:22.700Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|nmutex] Done
[20131218T17:44:22.700Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|29 inet-rpc|4986c26d-ced4-4153-a343-ef4f6a4add27|vhdd] Response: (omitted)
[20131218T17:44:22.701Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|30 inet-rpc||http] Request { frame = false; method = POST; uri = /internal; query = [  ]; content_length = [ 144 ]; transfer encoding = ; version = 1.0; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = ; user_agent = xen-api-libs/1.0 }
[20131218T17:44:22.701Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|30 inet-rpc||vhdd] Internal handler
[20131218T17:44:22.701Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|30 inet-rpc||vhdd] Call={"method": "Debug.check_junk", "params": [{"sr": "1", "vdi": "3aa413f6-76f3-4b22-9723-e49feda81d54", "current": [[[[0, 10]], 55]]}], "id": 1266}
[20131218T17:44:22.702Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|30 inet-rpc||vhdsm] Checking junk
[20131218T17:44:22.702Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|30 inet-rpc||vhdsm] Junk OK (1)
[20131218T17:44:22.702Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|30 inet-rpc||vhdd] Response: {"result": null, "error": null, "id": 0}
[20131218T17:44:22.703Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|31 inet-rpc||http] Request { frame = false; method = POST; uri = /internal; query = [  ]; content_length = [ 129 ]; transfer encoding = ; version = 1.0; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = ; user_agent = xen-api-libs/1.0 }
[20131218T17:44:22.703Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|31 inet-rpc||vhdd] Internal handler
[20131218T17:44:22.703Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|31 inet-rpc||vhdd] Call={"method": "Debug.check_junk", "params": [{"sr": "1", "vdi": "3aa413f6-76f3-4b22-9723-e49feda81d54", "current": []}], "id": 1267}
[20131218T17:44:22.703Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|31 inet-rpc||vhdsm] Checking junk
[20131218T17:44:22.703Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|31 inet-rpc||vhdsm] Junk OK (0)
[20131218T17:44:22.703Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|31 inet-rpc||vhdd] Response: {"result": null, "error": null, "id": 0}
[20131218T17:44:22.704Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 415 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:22.705Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:22.705Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.deactivate</methodName><params><param><value><struct><member><name>dbg</name><value>dbg</value></member><member><name>dp</name><value>3aa413f6-76f3-4b22-9723-e49feda81d54</value></member><member><name>sr</name><value>1</value></member><member><name>vdi</name><value>3aa413f6-76f3-4b22-9723-e49feda81d54</value></member></struct></value></param></params></methodCall>
[20131218T17:44:22.705Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|3bbd8f7c-84c9-42a9-b7c8-81a9d573b519|vhdsm] API call: VDI.deactivate sr=1 vdi=3aa413f6-76f3-4b22-9723-e49feda81d54
[20131218T17:44:22.705Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|3bbd8f7c-84c9-42a9-b7c8-81a9d573b519|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:22.705Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|3bbd8f7c-84c9-42a9-b7c8-81a9d573b519|nmutex] wait: reason=Adding to current_operations
[20131218T17:44:22.705Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|3bbd8f7c-84c9-42a9-b7c8-81a9d573b519|vhdSlave] Checking current ops
[20131218T17:44:22.705Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|3bbd8f7c-84c9-42a9-b7c8-81a9d573b519|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:22.705Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|3bbd8f7c-84c9-42a9-b7c8-81a9d573b519|nmutex] wait: reason=Deactivating
[20131218T17:44:22.705Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|3bbd8f7c-84c9-42a9-b7c8-81a9d573b519|tapdisk_listen] Unregistering 1/3aa413f6-76f3-4b22-9723-e49feda81d54
[20131218T17:44:22.706Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|3bbd8f7c-84c9-42a9-b7c8-81a9d573b519|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.706Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|3bbd8f7c-84c9-42a9-b7c8-81a9d573b519|nmutex] wait: reason=Executing with operation '"OpDeactivating"'
[20131218T17:44:22.706Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|3bbd8f7c-84c9-42a9-b7c8-81a9d573b519|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.706Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|3bbd8f7c-84c9-42a9-b7c8-81a9d573b519|nmutex] wait: reason=Update id_map
[20131218T17:44:22.706Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|3bbd8f7c-84c9-42a9-b7c8-81a9d573b519|locking] update_leaf: vdi_location=3aa413f6-76f3-4b22-9723-e49feda81d54
[20131218T17:44:22.706Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|3bbd8f7c-84c9-42a9-b7c8-81a9d573b519|nmutex] wait: reason=Finished op '"OpDeactivating"'. Removing from cur
[20131218T17:44:22.706Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|3bbd8f7c-84c9-42a9-b7c8-81a9d573b519|nmutex] About to broadcast
[20131218T17:44:22.706Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|3bbd8f7c-84c9-42a9-b7c8-81a9d573b519|nmutex] Done
[20131218T17:44:22.706Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|3bbd8f7c-84c9-42a9-b7c8-81a9d573b519|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:22.706Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|3bbd8f7c-84c9-42a9-b7c8-81a9d573b519|nmutex] wait: reason=Removing from current_operations
[20131218T17:44:22.706Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|3bbd8f7c-84c9-42a9-b7c8-81a9d573b519|vhdSlave] Removing my current op
[20131218T17:44:22.706Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|3bbd8f7c-84c9-42a9-b7c8-81a9d573b519|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:22.706Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|3bbd8f7c-84c9-42a9-b7c8-81a9d573b519|nmutex] About to broadcast
[20131218T17:44:22.706Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|3bbd8f7c-84c9-42a9-b7c8-81a9d573b519|nmutex] Done
[20131218T17:44:22.707Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|32 inet-rpc|3bbd8f7c-84c9-42a9-b7c8-81a9d573b519|vhdd] Response: (omitted)
[20131218T17:44:22.708Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 411 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:22.708Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:22.708Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.detach</methodName><params><param><value><struct><member><name>dbg</name><value>dbg</value></member><member><name>dp</name><value>3aa413f6-76f3-4b22-9723-e49feda81d54</value></member><member><name>sr</name><value>1</value></member><member><name>vdi</name><value>3aa413f6-76f3-4b22-9723-e49feda81d54</value></member></struct></value></param></params></methodCall>
[20131218T17:44:22.708Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|vhdsm] API call: VDI.detach sr=1 vdi=3aa413f6-76f3-4b22-9723-e49feda81d54
[20131218T17:44:22.708Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:22.708Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|nmutex] wait: reason=Adding to current_operations
[20131218T17:44:22.708Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|vhdSlave] Checking current ops
[20131218T17:44:22.708Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:22.709Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|nmutex] wait: reason=Checking we're attached
[20131218T17:44:22.709Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|nmutex] wait: reason=Deactivating tapdisk if necessary
[20131218T17:44:22.713Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.713Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|nmutex] wait: reason=Executing with operation '"OpDetaching"'
[20131218T17:44:22.714Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.714Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|nmutex] wait: reason=Update id_map
[20131218T17:44:22.714Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|locking] update_leaf: vdi_location=3aa413f6-76f3-4b22-9723-e49feda81d54
[20131218T17:44:22.714Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|nmutex] wait: reason=Getting VHD uid='0f8513df-e3d6-4642-b24c-6e68c935d1a3'
[20131218T17:44:22.714Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|vhdMaster_utils] Resizing VHD uid: 0f8513df-e3d6-4642-b24c-6e68c935d1a3
[20131218T17:44:22.714Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-c527f70a-35b1-414d-9fe1-2952e749b6ad
[20131218T17:44:22.714Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|mlvm] Using dm_name=047b4496-332c-467a-ba1b-b0ae5963831d (use_tmp=true)
[20131218T17:44:22.714Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|vhdutil] Dump size: overhead=4312576 phys_size=4312576 virtual_size=52428800 critical_size=56741376
[20131218T17:44:22.714Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|vhdutil] leaf_status: Some false reservation_type: Thin
[20131218T17:44:22.714Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-c527f70a-35b1-414d-9fe1-2952e749b6ad
[20131218T17:44:22.714Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|vhdMaster_utils] new_size=old_size=58720256. Not doing anything
[20131218T17:44:22.714Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|nmutex] wait: reason=Finished op '"OpDetaching"'. Removing from cur
[20131218T17:44:22.714Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|nmutex] About to broadcast
[20131218T17:44:22.715Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|nmutex] Done
[20131218T17:44:22.715Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|vhdSlave] s_mutex lock: with_op_inner
[20131218T17:44:22.715Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|nmutex] wait: reason=Adding to master_approved_ops
[20131218T17:44:22.715Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|vhdSlave] Checking current ops
[20131218T17:44:22.715Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|vhdSlave] Adding my current op to the current_ops hashtbl
[20131218T17:44:22.715Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|nmutex] wait: reason=Reloading attach info
[20131218T17:44:22.715Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--c527f70a--35b1--414d--9fe1--2952e749b6ad is now 0
[20131218T17:44:22.715Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|host] Removing LV=VG_XenStorage--1-VHD--c527f70a--35b1--414d--9fe1--2952e749b6ad
[20131218T17:44:22.715Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e is now 0
[20131218T17:44:22.715Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|host] Removing LV=VG_XenStorage--1-VHD--20c44869--83e6--4139--95f3--daa90c5fd32e
[20131218T17:44:22.715Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|nmutex] wait: reason=Removing attach info
[20131218T17:44:22.715Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:22.715Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|nmutex] wait: reason=Removing from master_approved_ops
[20131218T17:44:22.716Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|vhdSlave] Removing my current op
[20131218T17:44:22.716Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:22.716Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|nmutex] About to broadcast
[20131218T17:44:22.716Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|nmutex] Done
[20131218T17:44:22.716Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|vhdSlave] s_mutex lock: with_op_inner finally clause
[20131218T17:44:22.716Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|nmutex] wait: reason=Removing from current_operations
[20131218T17:44:22.716Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|vhdSlave] Removing my current op
[20131218T17:44:22.716Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|vhdSlave] Broadcasting to wake up other threads
[20131218T17:44:22.716Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|nmutex] About to broadcast
[20131218T17:44:22.716Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|nmutex] Done
[20131218T17:44:22.716Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|33 inet-rpc|517ef135-e3be-4e9e-ab27-f62391a8bd7a|vhdd] Response: (omitted)
[20131218T17:44:22.717Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 336 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:22.717Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:22.717Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.destroy</methodName><params><param><value><struct><member><name>dbg</name><value>delete_vdi</value></member><member><name>sr</name><value>1</value></member><member><name>vdi</name><value>3aa413f6-76f3-4b22-9723-e49feda81d54</value></member></struct></value></param></params></methodCall>
[20131218T17:44:22.718Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|vhdsm] API call: VDI.delete sr=1 vdi=3aa413f6-76f3-4b22-9723-e49feda81d54
[20131218T17:44:22.718Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|nmutex] wait: reason=Retrieving the hashtbl of attached hosts
[20131218T17:44:22.718Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.718Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|nmutex] wait: reason=Executing with operation '"OpDelete"'
[20131218T17:44:22.718Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.718Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|nmutex] wait: reason=Removing an ID from the mapping (3aa413f6-76f3-4b22-9723-e49feda81d54)
[20131218T17:44:22.718Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|lvmabs] operating on vg: VG_XenStorage-1 lv: id_to_leaf_mapping
[20131218T17:44:22.718Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|host] attach_lv: Got the mutex
[20131218T17:44:22.718Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|host] LV VG_XenStorage--1-id_to_leaf_mapping not attached: attaching. refcount now 1
[20131218T17:44:22.718Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|mlvm] Using dm_name=VG_XenStorage--1-id_to_leaf_mapping (use_tmp=false)
[20131218T17:44:22.718Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|host] attach_lv: released the mutex
[20131218T17:44:22.719Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|host] Remove LV: refcount for dm=VG_XenStorage--1-id_to_leaf_mapping is now 0
[20131218T17:44:22.719Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|host] Removing LV=VG_XenStorage--1-id_to_leaf_mapping
[20131218T17:44:22.719Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|nmutex] wait: reason=Getting VHD uid='0f8513df-e3d6-4642-b24c-6e68c935d1a3'
[20131218T17:44:22.719Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|vhdMaster_utils] Calling set_hidden on VHD uid=0f8513df-e3d6-4642-b24c-6e68c935d1a3
[20131218T17:44:22.719Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-c527f70a-35b1-414d-9fe1-2952e749b6ad
[20131218T17:44:22.719Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|host] attach_lv: Got the mutex
[20131218T17:44:22.719Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|host] LV VG_XenStorage--1-VHD--c527f70a--35b1--414d--9fe1--2952e749b6ad not attached: attaching. refcount now 1
[20131218T17:44:22.719Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|mlvm] Using dm_name=VG_XenStorage--1-VHD--c527f70a--35b1--414d--9fe1--2952e749b6ad (use_tmp=false)
[20131218T17:44:22.719Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|host] attach_lv: released the mutex
[20131218T17:44:22.719Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|vhdutil] query_size_vhd: Querying size of VHD 0f8513df-e3d6-4642-b24c-6e68c935d1a3
[20131218T17:44:22.719Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|vhdutil] query_size_vhd: get_phys_size returned 4312576
[20131218T17:44:22.719Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|vhdutil] query_size_vhd: first_allocated_block=None
[20131218T17:44:22.719Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|vhdutil] query_size_vhd: virtual_size=52428800
[20131218T17:44:22.740Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--c527f70a--35b1--414d--9fe1--2952e749b6ad is now 0
[20131218T17:44:22.778Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|host] Removing LV=VG_XenStorage--1-VHD--c527f70a--35b1--414d--9fe1--2952e749b6ad
[20131218T17:44:22.778Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|nmutex] wait: reason=Update_hidden ptr='["PVhd", "0f8513df-e3d6-4642-b24c-6e68c935d1a3"]' hidden=2
[20131218T17:44:22.779Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|nmutex] wait: reason=Removing VHD uid='0f8513df-e3d6-4642-b24c-6e68c935d1a3'
[20131218T17:44:22.779Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-c527f70a-35b1-414d-9fe1-2952e749b6ad
[20131218T17:44:22.779Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|mlvm] LVM REDO: 000000000065„•¦¾   -          H”	(VHD-c527f70a-35b1-414d-9fe1-2952e749b6ad
[20131218T17:44:22.779Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|nmutex] wait: reason=Finished op '"OpDelete"'. Removing from cur
[20131218T17:44:22.779Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|nmutex] About to broadcast
[20131218T17:44:22.779Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|nmutex] Done
[20131218T17:44:22.779Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|34 inet-rpc|3791912b-4213-4878-bd8f-99257927952b|vhdd] Response: (omitted)
[20131218T17:44:22.780Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 336 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:22.781Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:22.781Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>VDI.destroy</methodName><params><param><value><struct><member><name>dbg</name><value>delete_vdi</value></member><member><name>sr</name><value>1</value></member><member><name>vdi</name><value>a90416d8-2762-44e9-b9f0-139b3a0c4c56</value></member></struct></value></param></params></methodCall>
[20131218T17:44:22.781Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|vhdsm] API call: VDI.delete sr=1 vdi=a90416d8-2762-44e9-b9f0-139b3a0c4c56
[20131218T17:44:22.781Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|nmutex] wait: reason=Retrieving the hashtbl of attached hosts
[20131218T17:44:22.781Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.781Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|nmutex] wait: reason=Executing with operation '"OpDelete"'
[20131218T17:44:22.781Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|nmutex] wait: reason=Locked get leaf info
[20131218T17:44:22.781Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|nmutex] wait: reason=Removing an ID from the mapping (a90416d8-2762-44e9-b9f0-139b3a0c4c56)
[20131218T17:44:22.781Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|lvmabs] operating on vg: VG_XenStorage-1 lv: id_to_leaf_mapping
[20131218T17:44:22.781Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|host] attach_lv: Got the mutex
[20131218T17:44:22.781Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|host] LV VG_XenStorage--1-id_to_leaf_mapping not attached: attaching. refcount now 1
[20131218T17:44:22.781Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|mlvm] Using dm_name=VG_XenStorage--1-id_to_leaf_mapping (use_tmp=false)
[20131218T17:44:22.781Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|host] attach_lv: released the mutex
[20131218T17:44:22.782Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|host] Remove LV: refcount for dm=VG_XenStorage--1-id_to_leaf_mapping is now 0
[20131218T17:44:22.782Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|host] Removing LV=VG_XenStorage--1-id_to_leaf_mapping
[20131218T17:44:22.782Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|nmutex] wait: reason=Getting VHD uid='d5a0becd-8d46-4b2d-b45f-6e45c10b06bb'
[20131218T17:44:22.782Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|vhdMaster_utils] Calling set_hidden on VHD uid=d5a0becd-8d46-4b2d-b45f-6e45c10b06bb
[20131218T17:44:22.782Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-d7afdbb6-8482-4d84-b369-b98731fcbe7b
[20131218T17:44:22.782Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|host] attach_lv: Got the mutex
[20131218T17:44:22.782Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|host] LV VG_XenStorage--1-VHD--d7afdbb6--8482--4d84--b369--b98731fcbe7b not attached: attaching. refcount now 1
[20131218T17:44:22.782Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|mlvm] Using dm_name=VG_XenStorage--1-VHD--d7afdbb6--8482--4d84--b369--b98731fcbe7b (use_tmp=false)
[20131218T17:44:22.782Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|host] attach_lv: released the mutex
[20131218T17:44:22.782Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|vhdutil] query_size_vhd: Querying size of VHD d5a0becd-8d46-4b2d-b45f-6e45c10b06bb
[20131218T17:44:22.782Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|vhdutil] query_size_vhd: get_phys_size returned 4312576
[20131218T17:44:22.782Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|vhdutil] query_size_vhd: first_allocated_block=None
[20131218T17:44:22.782Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|vhdutil] query_size_vhd: virtual_size=52428800
[20131218T17:44:22.787Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|host] Remove LV: refcount for dm=VG_XenStorage--1-VHD--d7afdbb6--8482--4d84--b369--b98731fcbe7b is now 0
[20131218T17:44:22.787Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|host] Removing LV=VG_XenStorage--1-VHD--d7afdbb6--8482--4d84--b369--b98731fcbe7b
[20131218T17:44:22.787Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|nmutex] wait: reason=Update_hidden ptr='["PVhd", "d5a0becd-8d46-4b2d-b45f-6e45c10b06bb"]' hidden=2
[20131218T17:44:22.787Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|nmutex] wait: reason=Removing VHD uid='d5a0becd-8d46-4b2d-b45f-6e45c10b06bb'
[20131218T17:44:22.787Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|lvmabs] operating on vg: VG_XenStorage-1 lv: VHD-d7afdbb6-8482-4d84-b369-b98731fcbe7b
[20131218T17:44:22.787Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|mlvm] LVM REDO: 000000000065„•¦¾   -          I”	(VHD-d7afdbb6-8482-4d84-b369-b98731fcbe7b
[20131218T17:44:22.787Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|nmutex] wait: reason=Finished op '"OpDelete"'. Removing from cur
[20131218T17:44:22.788Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|nmutex] About to broadcast
[20131218T17:44:22.788Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|nmutex] Done
[20131218T17:44:22.788Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|35 inet-rpc|4348c952-269e-480a-82e8-03b10f3b4519|vhdd] Response: (omitted)
[20131218T17:44:22.791Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc||http] Request { frame = false; method = POST; uri = /lvmnew; query = [  ]; content_length = [ 250 ]; transfer encoding = ; version = 1.1; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = 127.0.0.1:4094; user_agent = ./ftests }
[20131218T17:44:22.791Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc||vhdd] path=lvmnew
[20131218T17:44:22.791Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc||vhdd] Request: <?xml version="1.0"?><methodCall><methodName>SR.detach</methodName><params><param><value><struct><member><name>dbg</name><value>detach_all</value></member><member><name>sr</name><value>1</value></member></struct></value></param></params></methodCall>
[20131218T17:44:22.792Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|f12a4359-7450-45b6-8469-09a22042cdf3|lvmabs] operating on vg: VG_XenStorage-1 lv: host_attachments
[20131218T17:44:22.792Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|f12a4359-7450-45b6-8469-09a22042cdf3|host] attach_lv: Got the mutex
[20131218T17:44:22.792Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|f12a4359-7450-45b6-8469-09a22042cdf3|host] LV VG_XenStorage--1-host_attachments not attached: attaching. refcount now 1
[20131218T17:44:22.792Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|f12a4359-7450-45b6-8469-09a22042cdf3|mlvm] Using dm_name=VG_XenStorage--1-host_attachments (use_tmp=false)
[20131218T17:44:22.792Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|f12a4359-7450-45b6-8469-09a22042cdf3|host] attach_lv: released the mutex
[20131218T17:44:22.792Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|f12a4359-7450-45b6-8469-09a22042cdf3|host] Remove LV: refcount for dm=VG_XenStorage--1-host_attachments is now 0
[20131218T17:44:22.792Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|f12a4359-7450-45b6-8469-09a22042cdf3|host] Removing LV=VG_XenStorage--1-host_attachments
[20131218T17:44:22.792Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|f12a4359-7450-45b6-8469-09a22042cdf3|mlvm] write_label_and_pv_header:
PV header:
pvh_id: ExEgaJ-3nYb-glBg-7Pem-Er1s-O6gd-LcDhN7
pvh_device_size: 1099511627776
pvh_areas1: {offset=10551296,size=0}
pvh_areas2: {offset=4096,size=10547200}

[20131218T17:44:22.793Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|f12a4359-7450-45b6-8469-09a22042cdf3|mlvm] Writing MDA header
[20131218T17:44:22.793Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|f12a4359-7450-45b6-8469-09a22042cdf3|mlvm] Writing: checksum: 632148082
magic:  LVM2 x[5A%r0N*>
version: 1
start: 4096
size: 10485760
raw_locns:[{offset:2048,size:1455,checksum:-1059961676,filler:0}]

[20131218T17:44:22.793Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|f12a4359-7450-45b6-8469-09a22042cdf3|host] remove_pv_id_info: Got mutex
[20131218T17:44:22.793Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|f12a4359-7450-45b6-8469-09a22042cdf3|host] remove_pv_id_info: Released mutex
[20131218T17:44:22.794Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|36 inet-rpc|f12a4359-7450-45b6-8469-09a22042cdf3|vhdd] Response: (omitted)
[20131218T17:44:22.795Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|37 inet-rpc||http] Request { frame = false; method = POST; uri = /internal; query = [  ]; content_length = [ 67 ]; transfer encoding = ; version = 1.0; cookie = [ = ]; task = ; subtask_of = ; content-type = ; host = ; user_agent = xen-api-libs/1.0 }
[20131218T17:44:22.795Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|37 inet-rpc||vhdd] Internal handler
[20131218T17:44:22.795Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|37 inet-rpc||vhdd] Call={"method": "Debug.die", "params": [{"restart": false}], "id": 1268}
[20131218T17:44:22.795Z|debug|testing-worker-linux-7-2-6507-linux-19-15659603|37 inet-rpc||vhdsm] Got instruction to die with restart=false
[20131218T17:44:22.798Z| info|testing-worker-linux-7-2-6507-linux-19-15659603|0||watchdog] received exit code 0. Not restarting.
